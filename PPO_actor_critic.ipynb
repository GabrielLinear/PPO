{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import collections\n",
    "from multiprocessing import Process\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/gabyc/Desktop/Reinforcment_TP/deep-reinforcement-learning/p2_continuous-control/Multi_agent/Reacher_Windows_x86_64/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    # Actor network \n",
    "    def __init__(self,input_size,nb_action):\n",
    "        super(Policy, self).__init__()\n",
    "        self.nb_action = nb_action\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.fc1 = nn.Linear(input_size,200)\n",
    "        self.fc2 = nn.Linear(200,75)\n",
    "        self.fc3 = nn.Linear(75,nb_action)\n",
    "        self.fc3bis = nn.Linear(75,nb_action)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = F.tanh(self.fc3(x)) # Tanh because action_values between -1 and 1.\n",
    "\n",
    "        sigma = torch.ones(self.nb_action,requires_grad=False).to(self.device)/2 \n",
    "        m = torch.distributions.normal.Normal(mu,sigma,False)# False, whereas constraint on mu = 0\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    # Critic network \n",
    "    def __init__(self,input_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,150)\n",
    "        self.fc2 = nn.Linear(150,50)\n",
    "        self.fc3 = nn.Linear(50,1) # 1 output -> Value estimate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return  F.relu(self.fc3(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_prob(policy,states,actions,device):\n",
    "    # The Gradient FLOW on action\n",
    "    # The Gradient fon't FLOW on state \n",
    "    # No Clipping.\n",
    "    Tab = []\n",
    "    Action_sample_tab = []\n",
    "    m = policy(states[0])\n",
    "    \n",
    "    proba = m.log_prob(actions[0])\n",
    "\n",
    "    # STORE\n",
    "    Tab.append(proba)\n",
    "    Action_sample_tab.append(actions[0])\n",
    "    \n",
    "    # Loop over the state and action (a,s)\n",
    "    for state_iter,action_iter in zip(states[1:],actions[1:]):\n",
    "        m = policy(state_iter)\n",
    "        proba = m.log_prob(action_iter) # Prob on the previous action but new policy\n",
    "   \n",
    "        # STORE\n",
    "        Tab.append(proba)\n",
    "        Action_sample_tab.append(action_iter)\n",
    "\n",
    "    return torch.stack(Tab),torch.stack(Action_sample_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(Delta_t,critic,device,policy, old_probs,actions, states, rewards,batch_size,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "    \n",
    "\n",
    "    \n",
    "    # Convert REWARD TO REWARD FUTURE\n",
    "    rewards = np.asarray(rewards)\n",
    "    reward_futur = np.zeros((rewards.shape[0],rewards.shape[1]))\n",
    "    longueur = rewards.shape[0] - 1\n",
    "    reward_futur[longueur] = rewards[longueur]\n",
    "    new_discount = 0\n",
    "    for i in range(1,rewards.shape[0]):\n",
    "        new_discount = discount**(longueur-i) \n",
    "        reward_futur[longueur-i] = reward_futur[longueur-(i-1)] + rewards[longueur-i]*new_discount\n",
    "        \n",
    "    # Compute normalized reward\n",
    "    mean = np.mean(reward_futur, axis=1)\n",
    "    std = np.std(reward_futur, axis=1)+1.0e-10\n",
    "    normalized_rewards = (reward_futur-mean[:, np.newaxis])/std[:, np.newaxis]\n",
    "    normalized_rewards = torch.from_numpy(normalized_rewards).float().to(device)\n",
    "    normalized_rewards = normalized_rewards.unsqueeze(2)\n",
    "    normalized_rewards = normalized_rewards.repeat(1, 1, old_probs.shape[2])\n",
    "    \n",
    "    \n",
    "    #Normalize At\n",
    "    Delta_t = Delta_t.detach()\n",
    "    Delta_t = Delta_t.repeat(1, 1, old_probs.shape[2])\n",
    "    Delta_t = (Delta_t- Delta_t.mean())/Delta_t.std()\n",
    "    \n",
    "    \n",
    "    new_prob,action_sample = New_prob(policy, states,actions,device)\n",
    "    \n",
    "    # Compute each \n",
    "    Fraction = torch.exp(new_prob-(old_probs+1e-10))\n",
    "    Cote1 = Delta_t*Fraction \n",
    "    Cote2 = Delta_t*torch.clamp(Fraction, 1-epsilon, 1+epsilon) \n",
    "    Cote1 = Cote1[:, :,:, None]\n",
    "    Cote2 = Cote2[:, :,:, None]\n",
    "    comp = torch.cat((Cote1, Cote2),3)\n",
    "    Gradient = torch.min(comp,3)[0].to(device) # Surrogate function\n",
    "\n",
    "\n",
    "    entropy = -(torch.exp(new_prob)*old_probs+1.e-10)+ \\\n",
    "        (1.0-torch.exp(new_prob))*(1.0-old_probs+1.e-10) # Entropy to enhance exploration\n",
    "\n",
    "    writer.add_scalar('Entropy',torch.mean(beta*(entropy)),iteration_all)\n",
    "    writer.add_scalar('Gradient',torch.mean(Gradient),iteration_all)\n",
    "    \n",
    "    MSE = TD_Training(critic,states,reward_futur,discount,device) # Critic network training\n",
    "    writer.add_scalar('Loss/Critic',MSE,iteration_all)\n",
    "\n",
    "    return -torch.mean(beta*(entropy) + Gradient)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_Training(Critic,states,reward,discount,device):\n",
    "    states = states.detach()\n",
    "    reward = torch.from_numpy(reward).detach()\n",
    "    value_loss = []\n",
    "    for st in states:\n",
    "        Valuet = Critic(st)\n",
    "        value_loss.append(Valuet)\n",
    "        \n",
    "    Loss = 0.5*(discount*reward.to(device).unsqueeze(2) - torch.stack(value_loss)).pow(2).mean() # Simple MSE Loss\n",
    "    optimizer_c.zero_grad()\n",
    "    Loss.backward()\n",
    "    optimizer_c.step()\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env,env_info,policy,device,tmax):\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    state = env_info.vector_observations # get the current state (for each agent)\n",
    "    states_tab , action_tab, reward_tab, prob_tab = [],[],[], []\n",
    "    t = 0\n",
    "    while True:\n",
    "        state = torch.from_numpy(state).to(device)\n",
    "        policy.eval()\n",
    "        with torch.no_grad(): # Everything with torch no grad.\n",
    "            m = policy(state) \n",
    "\n",
    "        \n",
    "            # Sample maybe on gradient as to check that\n",
    "            sample = m.sample()\n",
    "            action_tab.append(sample) # No clip and store\n",
    "\n",
    "            # Proba not on clip and detach from Gradient.\n",
    "            proba = m.log_prob(sample)\n",
    "            #proba = torch.exp(proba) #Proba on CUDA no detach\n",
    "            \n",
    "            # Interact with the environment \n",
    "            sample = torch.clip(sample.detach().cpu(), -1, 1) # CLIP BEFORE TAKING THE PROBA OR AFTER?\n",
    "            sample = sample.numpy()\n",
    "\n",
    "\n",
    "            # Step the environment\n",
    "            env_info = env.step(sample)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "            # Store values\n",
    "            prob_tab.append(proba)\n",
    "            reward_tab.append(np.asarray(rewards))\n",
    "            states_tab.append(state)\n",
    "\n",
    "            # BREAK IF END OF THE EPISODE\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            if t >= tmax:\n",
    "                break\n",
    "            state = next_states\n",
    "            t +=1\n",
    "    writer.add_histogram('MU/Sample_mu_action0',torch.mean(torch.stack(action_tab)[:,:,0],axis=1),iteration_all)\n",
    "    writer.add_histogram('MU/Sample_mu_action1',torch.mean(torch.stack(action_tab)[:,:,1],axis=1),iteration_all)\n",
    "    writer.add_histogram('MU/Sample_mu_action2',torch.mean(torch.stack(action_tab)[:,:,2],axis=1),iteration_all)\n",
    "    writer.add_histogram('MU/Sample_mu_action3',torch.mean(torch.stack(action_tab)[:,:,3],axis=1),iteration_all)\n",
    "    return states_tab, action_tab, reward_tab,prob_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_evaluation(Critic,states,reward,discount,device):\n",
    "    # Calculate TD error during the evaluation step\n",
    "    Delta_t = []\n",
    "    Tab = []\n",
    "    Critic.eval()\n",
    "    with torch.no_grad(): \n",
    "        Valuet = Critic(states[0])\n",
    "    \n",
    "        for rw,st in zip(reward[0:],states[1:]):\n",
    "            Valuetplus1 = Critic(st)\n",
    "            Tab.append(Valuetplus1)\n",
    "            TD_error = torch.from_numpy(rw).to(device).unsqueeze(1) + discount*Valuetplus1 - Valuet #TD ERROR\n",
    "            Delta_t.append(TD_error)\n",
    "            \n",
    "            Valuet = Valuetplus1\n",
    "    writer.add_histogram('Values',torch.mean(torch.stack(Tab),axis=1),e)\n",
    "    return torch.stack(Delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAE_evaluation(Delta_t,discount,lambd):\n",
    "    # GAE Function adapted from https://github.com/numblr/drlnd-cocontrol\n",
    "    flipped = torch.flip(Delta_t, dims=(0,))\n",
    "    result = torch.zeros_like(flipped)\n",
    "    result[0,:,:] = flipped[0, :, :]\n",
    "    for i in range(1, flipped.size()[0]):\n",
    "        result[i,:,:] = discount * lambd * result[i-1,:,:] + flipped[i,:,:]\n",
    "\n",
    "    return torch.flip(result, dims=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]  \n",
    "states = env_info.vector_observations # get the current state (for each agent\n",
    "num_agents = len(states)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "nb_states = len(states[0])\n",
    "action_size = brain.vector_action_space_size\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = Policy(nb_states,action_size).to(device) # Policy network\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2e-4)\n",
    "critic = Critic(nb_states).to(device) # Critic network\n",
    "optimizer_c = optim.Adam(critic.parameters(), lr=2e-4)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabyc\\anaconda3\\envs\\Navigation3\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "###################################################### MAIN_CODE #################################################\n",
    "# training loop max iterations\n",
    "episode = 5000\n",
    "\n",
    "\n",
    "tmax = 1000\n",
    "discount_rate = .9997\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "SGD_epoch = 8\n",
    "batch_size = 64\n",
    "lambd = 0.95\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "writer.add_text(\"CONFIG\",\"aleatoire :\" + str(aleatoire) + \"tmax :\" + str(tmax) + \"batch_size :\" + str(batch_size) + \"discount_rate :\" + str(discount_rate) + \"epsilon\" + str(epsilon)+ \"beta\" + str(beta) + \"SGD_epoch :\" + str(SGD_epoch) + \"lambd :\" + str(lambd) + \"lr : 2e-4 x2\")\n",
    "iteration_all = 0\n",
    "for e in range(episode):\n",
    "    \n",
    "    # EVALUATION STEP\n",
    "    # collect trajectories\n",
    "    states, actions, rewards,prob = collect_trajectories(env,env_info, policy,device,tmax)\n",
    "    total_rewards = np.mean(np.sum(rewards,axis=0))\n",
    "    \n",
    "    # Compute advantages estimate\n",
    "    Delta_t = TD_evaluation(critic,states,rewards,discount_rate,device)\n",
    "    writer.add_scalar('DeltaT',torch.mean(Delta_t),iteration_all)\n",
    "    Delta_t = GAE_evaluation(Delta_t,discount_rate,lambd)\n",
    "    writer.add_scalar('Advantage',torch.mean(Delta_t),iteration_all)\n",
    "    \n",
    "    states = torch.stack(states)[:-1]\n",
    "    actions = torch.stack(actions)[:-1]\n",
    "    prob = torch.stack(prob)[:-1]\n",
    "    rewards = np.asarray(rewards)[:-1]\n",
    "    \n",
    "    # TRAINING STEP\n",
    "    indices = torch.split(torch.from_numpy(np.arange(0,states.shape[0],1)),batch_size,0) # Make chunk of the trajectory\n",
    "    for epoch in range(SGD_epoch):\n",
    "        # TRAINING OVER THE BATCH SIZE\n",
    "        for chunks in indices:\n",
    "            iteration_all += 1\n",
    "            chunk = chunks.long()\n",
    "            chunk_numpy = chunk.numpy().astype('int')\n",
    "\n",
    "            states_chunk = states[chunk]\n",
    "            actions_chunk = actions[chunk]\n",
    "            prob_chunk = prob[chunk]\n",
    "            rewards_chunk = rewards[chunk_numpy]\n",
    "            Delta_t_chunk = Delta_t[chunk]\n",
    "            rewards_chunk = rewards_chunk.tolist()\n",
    "            \n",
    "            L = clipped_surrogate(Delta_t_chunk,critic,device,policy,prob_chunk,actions_chunk, states_chunk, rewards_chunk,batch_size, epsilon=epsilon, beta=beta)\n",
    "            optimizer.zero_grad()\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Loss/Policy',L,iteration_all)\n",
    "            del L\n",
    "    writer.add_scalar('Score',total_rewards,e)\n",
    "            \n",
    "    mean_rewards.append(total_rewards)\n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"################################\")\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,total_rewards))\n",
    "        print(total_rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'PPO_critic_stable.pth')\n",
    "torch.save(critic.state_dict(), 'PPO_actor_stable.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Navigation3",
   "language": "python",
   "name": "navigation3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
