{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import collections\n",
    "from multiprocessing import Process\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/gabyc/Desktop/Reinforcment_TP/deep-reinforcement-learning/p2_continuous-control/Multi_agent/Reacher_Windows_x86_64/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a42d4a",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,nb_action):\n",
    "        super(Policy, self).__init__()\n",
    "        self.nb_action = nb_action\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.fc1 = nn.Linear(input_size,150)\n",
    "        self.fc2 = nn.Linear(150,75)\n",
    "        self.fc3 = nn.Linear(75,nb_action)\n",
    "        self.sigma = torch.ones(self.nb_action,requires_grad=False).to(self.device)/2\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = F.tanh(self.fc3(x)) # Tanh because action_values between -1 and 1.\n",
    "        #sigma = F.softplus(self.fc3bis(x))# Activation to stay always >= 0\n",
    "        #sigma = torch.clamp(sigma,0.001) # Activation to stay always > 0\n",
    "        sigma = self.sigma\n",
    "        m = torch.distributions.normal.Normal(mu,sigma,False) # False, whereas constraint on mu = 0\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,150)\n",
    "        self.fc2 = nn.Linear(150,50)\n",
    "        self.fc3 = nn.Linear(50,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return  F.relu(self.fc3(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15fb4dc",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env,env_info,policy,device,tmax):\n",
    "    # DEAL WITH THAT OLD_PROB AND ACTION ARE DIFFERENT NOW.\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    state = env_info.vector_observations # get the current state (for each agent)\n",
    "    states_tab , action_tab, reward_tab, prob_tab = [],[],[], []\n",
    "    t = 0\n",
    "    while True:\n",
    "        state = torch.from_numpy(state).to(device)\n",
    "        policy.eval()\n",
    "        with torch.no_grad(): # Everything with torch no grad.\n",
    "            #proba,action_sample,mu = policy(state) # Batch of 21\n",
    "            m = policy(state) \n",
    "\n",
    "        \n",
    "            # Sample maybe on gradient as to check that\n",
    "            sample = m.sample() \n",
    "            action_tab.append(sample) # No clip and store\n",
    "\n",
    "            # Proba not on clip and detach from Gradient.\n",
    "            proba = m.log_prob(sample)\n",
    "            #proba = torch.exp(proba) #Proba on CUDA no detach\n",
    "            \n",
    "            # Interact with the environment \n",
    "            sample = torch.clip(sample.detach().cpu(), -1, 1) # CLIP BEFORE TAKING THE PROBA OR AFTER?\n",
    "            sample = sample.numpy()\n",
    "\n",
    "\n",
    "            # Step the environment\n",
    "            env_info = env.step(sample)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "            # Store values\n",
    "            prob_tab.append(proba)\n",
    "            reward_tab.append(np.asarray(rewards))\n",
    "            states_tab.append(state)\n",
    "\n",
    "            # BREAK IF END OF THE EPISODE\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            if t >= tmax:\n",
    "                break\n",
    "            state = next_states\n",
    "            t +=1\n",
    "    return states_tab, action_tab, reward_tab,prob_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_evaluation(Critic,states,reward,discount,device):\n",
    "    Delta_t = []\n",
    "    Critic.eval()\n",
    "    with torch.no_grad(): \n",
    "        Valuet = Critic(states[0])\n",
    "    \n",
    "        for rw,st in zip(reward[1:],states[1:]):\n",
    "            Valuetplus1 = Critic(st)\n",
    "            TD_error = torch.from_numpy(rw).to(device).unsqueeze(1) + discount*Valuetplus1 - Valuet\n",
    "            Delta_t.append(TD_error)\n",
    "            \n",
    "            Valuet = Valuetplus1\n",
    "            \n",
    "    return torch.stack(Delta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6b1e0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate_critic(device,estimator_At,policy, old_probs,actions, states, rewards,batch_size\n",
    "                      ,critic,discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "    \n",
    "    old_probs = torch.stack(old_probs)\n",
    "    \n",
    "    \n",
    "    # Convert REWARD TO REWARD FUTURE\n",
    "    rewards = np.asarray(rewards)\n",
    "    #rewards = torch.from_numpy(rewards)\n",
    "    reward_futur = np.zeros((rewards.shape[0],rewards.shape[1]))\n",
    "    longueur = rewards.shape[0] - 1\n",
    "    reward_futur[longueur] = rewards[longueur]\n",
    "    new_discount = 0\n",
    "    for i in range(1,rewards.shape[0]):\n",
    "        new_discount = discount**(longueur-i) \n",
    "        reward_futur[longueur-i] = reward_futur[longueur-(i-1)] + rewards[longueur-i]*new_discount\n",
    "        \n",
    "    # Compute normalize reward\n",
    "    #mean = np.mean(reward_futur, axis=1)\n",
    "    #std = np.std(reward_futur, axis=1)+1.0e-10\n",
    "    #normalized_rewards = (reward_futur-mean[:, np.newaxis])/std[:, np.newaxis]\n",
    "    #normalized_rewards = torch.from_numpy(normalized_rewards).float().to(device)\n",
    "    #normalized_rewards = normalized_rewards.unsqueeze(2)\n",
    "    #normalized_rewards = normalized_rewards.repeat(1, 1, old_probs.shape[2])\n",
    "    \n",
    "    estimator_At = estimator_At.detach()\n",
    "    estimator_At = estimator_At.repeat(1, 1, old_probs.shape[2])\n",
    "    #print(estimator_At.shape)\n",
    "    mean = estimator_At.mean(dim=1)\n",
    "    std = estimator_At.std(dim=1)+1.0e-10\n",
    "    mean = mean.unsqueeze(1).repeat(1,estimator_At.shape[1],1)\n",
    "    std = std.unsqueeze(1).repeat(1,estimator_At.shape[1],1)\n",
    "    #print(mean.shape)\n",
    "    estimator_At = (estimator_At-mean)/std\n",
    "    \n",
    "    \n",
    "    ### SHUFFLE AND MAKING CHUNK ##\n",
    "    indexes = torch.randperm(old_probs.shape[0])\n",
    "    indexes_numpy = indexes.numpy().astype('int')\n",
    "    \n",
    "    #states = np.asarray(states)[indexes_numpy]\n",
    "    #actions = np.asarray(actions)[indexes_numpy]\n",
    "    #normalized_rewards = normalized_rewards[indexes]\n",
    "    #old_probs = old_probs[indexes]\n",
    "    Nb_split = int(old_probs.shape[0]/batch_size)\n",
    "    \n",
    "\n",
    "    indices = torch.split(torch.from_numpy(np.arange(0,estimator_At.shape[0],1)),batch_size,0)\n",
    "    \n",
    "    for chunks in indices:\n",
    "        chunk = chunks.long()\n",
    "        chunk_numpy = chunk.numpy().astype('int')\n",
    "\n",
    "        states_chunk = torch.stack(states)[chunk] \n",
    "        rewards_chunk = rewards[chunk]\n",
    "        reward_futur_chunk = reward_futur[chunk_numpy]\n",
    "        actions_chunk =  torch.stack(actions)[chunk]\n",
    "        #normalized_rewards_chunk = normalized_rewards[chunk]\n",
    "        old_prob_chunk = old_probs[chunk]\n",
    "        estimator_At_chunk = estimator_At[chunk]\n",
    "        \n",
    "        new_prob_chunk,action_sample_chunk = New_prob(policy, states_chunk,actions_chunk,device)\n",
    "        \n",
    "        # Normalisation des At\n",
    "\n",
    "        # Compute each \n",
    "        Fraction = torch.exp(new_prob_chunk-(old_prob_chunk+1e-10))\n",
    "        Cote1 = estimator_At_chunk*Fraction #*(action_sample-mu) \n",
    "        Cote2 = estimator_At_chunk*torch.clamp(Fraction, 1-epsilon, 1+epsilon) #*(action_sample-mu)\n",
    "        Cote1 = Cote1[:, :,:, None]\n",
    "        Cote2 = Cote2[:, :,:, None]\n",
    "        comp = torch.cat((Cote1, Cote2),3)\n",
    "        Gradient = torch.min(comp,3)[0].to(device)\n",
    "        #print(\"There is Nan Gradient\")\n",
    "        #print(torch.isnan(Gradient).any())\n",
    "        #print(Gradient)\n",
    "\n",
    "\n",
    "        entropy = -(torch.exp(new_prob_chunk)*old_prob_chunk+1.e-10)+ \\\n",
    "            (1.0-torch.exp(new_prob_chunk))*(1.0-old_prob_chunk+1.e-10) # Non definit si une valeur est inférieure à 0\n",
    "        #print(\"There is Nan entropy\") \n",
    "        #print(torch.isnan(entropy).any())\n",
    "        #print(torch.mean(beta*(entropy) + Gradient))\n",
    "        L = - torch.mean(beta*(entropy) + Gradient)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "        # Change with Reward : rewards\n",
    "        TD_Training(critic,states_chunk,reward_futur_chunk,discount,device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a63682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_prob(policy,states,actions,device):\n",
    "    # The Gradient FLOW on action\n",
    "    # The Gradient fon't FLOW on state yet\n",
    "    # No Clipping.\n",
    "    Tab = []\n",
    "    Action_sample_tab = []\n",
    "    m = policy(states[0])\n",
    "    \n",
    "    proba = m.log_prob(actions[0])\n",
    "    #probab = torch.exp(proba)\n",
    "    #probab = torch.clamp(probab,0.001) ## Don't why there is negative Probability\n",
    "    # Maybe deal with the Log without going to the exponential because of numeric diff\n",
    "    \n",
    "    # MAYBE CLIPPING AND MAYBE STILL SAMPLE SOMETHING TO DO (At -at)\n",
    "    #action_sample = torch.clip(sample.detach(), -1, 1)\n",
    "    #sample = m.sample()#.detach()\n",
    "    \n",
    "    # STORE\n",
    "    Tab.append(proba)\n",
    "    Action_sample_tab.append(actions[0])\n",
    "    \n",
    "    # Loop over the state and action (a,s)\n",
    "    for state_iter,action_iter in zip(states[1:],actions[1:]):\n",
    "        m = policy(state_iter)\n",
    "        #sample = m.sample()#.detach()\n",
    "        proba = m.log_prob(action_iter) # Prob on the previous action but new policy\n",
    "        #probab = torch.exp(proba)\n",
    "        #probab = torch.clamp(probab,0.001)\n",
    "        \n",
    "        # STORE\n",
    "        Tab.append(proba)\n",
    "        Action_sample_tab.append(action_iter)\n",
    "\n",
    "    return torch.stack(Tab),torch.stack(Action_sample_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69329cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_Training(Critic,states,reward,discount,device):\n",
    "    states = states.detach()\n",
    "    reward = torch.from_numpy(reward).detach()\n",
    "    value_loss = []\n",
    "    for st in states:\n",
    "        Valuet = Critic(st)\n",
    "        value_loss.append(Valuet)\n",
    "        \n",
    "    Loss = 0.5 *(discount*reward.to(device).unsqueeze(2) - torch.stack(value_loss)).pow(2).mean()\n",
    "    #Loss = 0.5 *(discount*reward[:,:,0].to(device).unsqueeze(2) - torch.stack(value_loss)).pow(2).mean()\n",
    "    #print(Loss)\n",
    "    optimizer.zero_grad()\n",
    "    Loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db19d74",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]  \n",
    "states = env_info.vector_observations # get the current state (for each agent\n",
    "num_agents = len(states)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "nb_states = len(states[0])\n",
    "action_size = brain.vector_action_space_size\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = Policy(nb_states,action_size).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "critic = Critic(nb_states).to(device)\n",
    "optimizer = optim.Adam(critic.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### MAIN_CODE #################################################\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "#!pip install progressbar\n",
    "#import progressbar as pb\n",
    "#widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "#timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 1200\n",
    "SGD_epoch = 1\n",
    "batch_size = 400\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    states, actions, rewards,prob = collect_trajectories_critic(env,env_info, policy,device,tmax)\n",
    "    total_rewards = np.mean(rewards)\n",
    "    print(total_rewards)\n",
    "    \n",
    "    #Delta_t = Online_TD_evaluation(critic,states,rewards,discount_rate,device)\n",
    "    Delta_t = TD_evaluation(critic,states,rewards,discount_rate,device)\n",
    "    #print(Delta_t.shape)\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        clipped_surrogate_critic(device,Delta_t,policy, prob,actions, states, rewards,batch_size,critic, epsilon=epsilon, beta=beta)\n",
    "        #L.requires_grad_() # I needed to do that to compute something but maybe that means that there is a bug.\n",
    "\n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.999\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"################################\")\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    #timer.update(e+1)\n",
    "    \n",
    "#timer.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Navigation2",
   "language": "python",
   "name": "navigation2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
