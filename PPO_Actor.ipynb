{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DIFFERENCE BETWEEN PONG AND OUR ENVIRONMENT ##############################\n",
    "# num_agent : 20 -> 20 agent\n",
    "# Size of each action : 4 -> One action is size(4) AND continuous.\n",
    "# Difference Not (1) -> But\n",
    "# In PONG : 1 last output probability of action 5 or 4.\n",
    "#     # convert states to policy (or probability)\n",
    "# new_probs = pong_utils.states_to_prob(policy, states)\n",
    "# new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "# outi​={xi​yi​​if conditioni​otherwise​}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/gabyc/Desktop/Reinforcment_TP/deep-reinforcement-learning/p2_continuous-control/Multi_agent/Reacher_Windows_x86_64/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import collections\n",
    "from multiprocessing import Process\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,nb_action):\n",
    "        super(Policy, self).__init__()\n",
    "        self.nb_action = nb_action\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.fc1 = nn.Linear(input_size,150)\n",
    "        self.fc2 = nn.Linear(150,75)\n",
    "        self.fc3 = nn.Linear(75,nb_action)\n",
    "        self.fc3bis = nn.Linear(75,nb_action)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = F.tanh(self.fc3(x)) # Tanh because action_values between -1 and 1.\n",
    "        #sigma = F.softplus(self.fc3bis(x))# Activation to stay always >= 0\n",
    "        #sigma = torch.clamp(sigma,0.001) # Activation to stay always > 0\n",
    "        sigma = torch.ones(self.nb_action,requires_grad=False).to(self.device)\n",
    "        m = torch.distributions.normal.Normal(mu,sigma,False) # False, whereas constraint on mu = 0\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_prob(policy,states,actions,device):\n",
    "    # The Gradient FLOW on action\n",
    "    # The Gradient fon't FLOW on state yet\n",
    "    # No Clipping.\n",
    "    Tab = []\n",
    "    Action_sample_tab = []\n",
    "    m = policy(states[0])\n",
    "    \n",
    "    proba = m.log_prob(actions[0])\n",
    "    #probab = torch.exp(proba)\n",
    "    #probab = torch.clamp(probab,0.001) ## Don't why there is negative Probability\n",
    "    # Maybe deal with the Log without going to the exponential because of numeric diff\n",
    "    \n",
    "    # MAYBE CLIPPING AND MAYBE STILL SAMPLE SOMETHING TO DO (At -at)\n",
    "    #action_sample = torch.clip(sample.detach(), -1, 1)\n",
    "    #sample = m.sample()#.detach()\n",
    "    \n",
    "    # STORE\n",
    "    Tab.append(proba)\n",
    "    Action_sample_tab.append(actions[0])\n",
    "    \n",
    "    # Loop over the state and action (a,s)\n",
    "    for state_iter,action_iter in zip(states[1:],actions[1:]):\n",
    "        m = policy(state_iter)\n",
    "        #sample = m.sample()#.detach()\n",
    "        proba = m.log_prob(action_iter) # Prob on the previous action but new policy\n",
    "        #probab = torch.exp(proba)\n",
    "        #probab = torch.clamp(probab,0.001)\n",
    "        \n",
    "        # STORE\n",
    "        Tab.append(proba)\n",
    "        Action_sample_tab.append(action_iter)\n",
    "\n",
    "    return torch.stack(Tab),torch.stack(Action_sample_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(device,policy, old_probs,actions, states, rewards,batch_size,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "    \n",
    "    old_probs = torch.stack(old_probs)\n",
    "    \n",
    "    # Convert REWARD TO REWARD FUTURE\n",
    "    rewards = np.asarray(rewards)\n",
    "    reward_futur = np.zeros((rewards.shape[0],rewards.shape[1]))\n",
    "    longueur = rewards.shape[0] - 1\n",
    "    reward_futur[longueur] = rewards[longueur]\n",
    "    new_discount = 0\n",
    "    for i in range(1,rewards.shape[0]):\n",
    "        new_discount = discount**(longueur-i) \n",
    "        reward_futur[longueur-i] = reward_futur[longueur-(i-1)] + rewards[longueur-i]*new_discount\n",
    "        \n",
    "    # Compute normalize reward\n",
    "    mean = np.mean(reward_futur, axis=1)\n",
    "    std = np.std(reward_futur, axis=1)+1.0e-10\n",
    "    normalized_rewards = (reward_futur-mean[:, np.newaxis])/std[:, np.newaxis]\n",
    "    normalized_rewards = torch.from_numpy(normalized_rewards).float().to(device)\n",
    "    normalized_rewards = normalized_rewards.unsqueeze(2)\n",
    "    normalized_rewards = normalized_rewards.repeat(1, 1, old_probs.shape[2])\n",
    "    \n",
    "    ### SHUFFLE AND MAKING CHUNK ##\n",
    "    indexes = torch.randperm(old_probs.shape[0])\n",
    "    indexes_numpy = indexes.numpy().astype('int')\n",
    "    \n",
    "    #states = np.asarray(states)[indexes_numpy]\n",
    "    #actions = np.asarray(actions)[indexes_numpy]\n",
    "    #normalized_rewards = normalized_rewards[indexes]\n",
    "    #old_probs = old_probs[indexes]\n",
    "    Nb_split = int(old_probs.shape[0]/batch_size)\n",
    "    \n",
    "    indices = torch.split(torch.from_numpy(np.arange(0,old_probs.shape[0],1)),batch_size,0)\n",
    "    \n",
    "    for chunks in indices:\n",
    "        chunk = chunks.long()\n",
    "        chunk_numpy = chunk.numpy().astype('int')\n",
    "        \n",
    "        states_chunk = torch.stack(states)[chunk]\n",
    "        actions_chunk = torch.stack(actions)[chunk]\n",
    "        normalized_rewards_chunk = normalized_rewards[chunk]\n",
    "        old_prob_chunk = old_probs[chunk]\n",
    "        \n",
    "        new_prob_chunk,action_sample_chunk = New_prob(policy, states_chunk,actions_chunk,device)\n",
    "    \n",
    "        # Compute each \n",
    "        Fraction = torch.exp(new_prob_chunk-(old_prob_chunk+1e-10))\n",
    "        Cote1 = normalized_rewards_chunk*Fraction #*(action_sample-mu) \n",
    "        Cote2 = normalized_rewards_chunk*torch.clamp(Fraction, 1-epsilon, 1+epsilon) #*(action_sample-mu)\n",
    "        Cote1 = Cote1[:, :,:, None]\n",
    "        Cote2 = Cote2[:, :,:, None]\n",
    "        comp = torch.cat((Cote1, Cote2),3)\n",
    "        Gradient = torch.min(comp,3)[0].to(device)\n",
    "        #print(\"There is Nan Gradient\")\n",
    "        #print(torch.isnan(Gradient).any())\n",
    "        #print(Gradient)\n",
    "\n",
    "\n",
    "        entropy = -(torch.exp(new_prob_chunk)*old_prob_chunk+1.e-10)+ \\\n",
    "            (1.0-torch.exp(new_prob_chunk))*(1.0-old_prob_chunk+1.e-10) # Non definit si une valeur est inférieure à 0\n",
    "        #print(\"There is Nan entropy\") \n",
    "        #print(torch.isnan(entropy).any())\n",
    "        #print(torch.mean(beta*(entropy) + Gradient))\n",
    "        L = - torch.mean(beta*(entropy) + Gradient)\n",
    "        #print(L)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env,env_info,policy,device,tmax):\n",
    "    # DEAL WITH THAT OLD_PROB AND ACTION ARE DIFFERENT NOW.\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    state = env_info.vector_observations # get the current state (for each agent)\n",
    "    states_tab , action_tab, reward_tab, prob_tab = [],[],[], []\n",
    "    t = 0\n",
    "    while True:\n",
    "        state = torch.from_numpy(state).to(device)\n",
    "        policy.eval()\n",
    "        with torch.no_grad(): # Everything with torch no grad.\n",
    "            #proba,action_sample,mu = policy(state) # Batch of 21\n",
    "            m = policy(state) \n",
    "\n",
    "        \n",
    "            # Sample maybe on gradient as to check that\n",
    "            sample = m.sample() \n",
    "            action_tab.append(sample) # No clip and store\n",
    "\n",
    "            # Proba not on clip and detach from Gradient.\n",
    "            proba = m.log_prob(sample)\n",
    "            #proba = torch.exp(proba) #Proba on CUDA no detach\n",
    "            \n",
    "            # Interact with the environment \n",
    "            sample = torch.clip(sample.detach().cpu(), -1, 1) # CLIP BEFORE TAKING THE PROBA OR AFTER?\n",
    "            sample = sample.numpy()\n",
    "\n",
    "\n",
    "            # Step the environment\n",
    "            env_info = env.step(sample)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "            # Store values\n",
    "            prob_tab.append(proba)\n",
    "            reward_tab.append(np.asarray(rewards))\n",
    "            states_tab.append(state)\n",
    "\n",
    "            # BREAK IF END OF THE EPISODE\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            if t >= tmax:\n",
    "                break\n",
    "            state = next_states\n",
    "            t +=1\n",
    "    return states_tab, action_tab, reward_tab,prob_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]  \n",
    "states = env_info.vector_observations # get the current state (for each agent\n",
    "num_agents = len(states)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "nb_states = len(states[0])\n",
    "action_size = brain.vector_action_space_size\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = Policy(nb_states,action_size).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4875621334533785e-06\n",
      "0.0012313432560594223\n",
      "0.00091293530297739\n",
      "0.0006716417760324122\n",
      "0.00020202019750469862\n",
      "0.0008980099301766697\n",
      "0.0010124377883155252\n",
      "0.0009004974923101231\n",
      "0.0011094527115202068\n",
      "0.0013959390550927462\n",
      "0.0008930348059097629\n",
      "0.0011716417648565413\n",
      "0.0009950248533813514\n",
      "0.0006542288410982386\n",
      "0.0008959390662686171\n",
      "0.0005771144149611839\n",
      "0.001077114403785313\n",
      "0.0018805969728907543\n",
      "0.0014577114102036798\n",
      "0.0008324872910371287\n",
      "################################\n",
      "Episode: 20, score: 0.000832\n",
      "0.0008324872910371287\n",
      "0.0010099502261820717\n",
      "0.0013109452443299306\n",
      "0.0007860696341712676\n",
      "0.0010721392795184062\n",
      "0.0010939086049908612\n",
      "0.0007164178944345731\n",
      "0.00128109449872849\n",
      "0.0011766168891234482\n",
      "0.0013830845462000786\n",
      "0.001253807078574212\n",
      "0.0009577114213795508\n",
      "0.0006019900362957177\n",
      "0.0011318407707212872\n",
      "0.0012487561909935961\n",
      "0.001479695398398311\n",
      "0.0013980099190007988\n",
      "0.0017338308070170049\n",
      "0.0009751243563137245\n",
      "0.0009477611728457373\n",
      "0.0010329949007686322\n",
      "################################\n",
      "Episode: 40, score: 0.001033\n",
      "0.0010329949007686322\n",
      "0.001253731315260503\n",
      "0.0021069651270350116\n",
      "0.001557213895541815\n",
      "0.0014676616587374935\n",
      "0.0010710659659075254\n",
      "0.0009975124155148047\n",
      "0.0016393034459457766\n",
      "0.001149253705655461\n",
      "0.0021094526891684653\n",
      "0.001517766463537204\n",
      "0.0008706467467086825\n",
      "0.0011716417648565413\n",
      "0.0013358208656644643\n",
      "0.0024527362635850315\n",
      "0.0012131979424260593\n",
      "0.0015970148896770691\n",
      "0.0009577114213795508\n",
      "0.001629353197411963\n",
      "0.0017960198603533394\n",
      "0.0012918781437131052\n",
      "################################\n",
      "Episode: 60, score: 0.001292\n",
      "0.0012918781437131052\n",
      "0.0011044775872533002\n",
      "0.0012363183803263292\n",
      "0.0015124377771396542\n",
      "0.001064676593118046\n",
      "0.0012208121554538381\n",
      "0.0009552238592460974\n",
      "0.0007562188885698271\n",
      "0.001216417883258702\n",
      "0.001601990013943976\n",
      "0.0011395938831575327\n",
      "0.001228855693925969\n",
      "0.0006393034682975183\n",
      "0.0010597014688511393\n",
      "0.0014328357888691461\n",
      "0.001479695398398311\n",
      "0.001497512404338934\n",
      "0.0017786069254191656\n",
      "0.002236318357974587\n",
      "0.001753731304084632\n",
      "0.0014822334694075704\n",
      "################################\n",
      "Episode: 80, score: 0.001482\n",
      "0.0014822334694075704\n",
      "0.0013781094219331718\n",
      "0.0013830845462000786\n",
      "0.0015746268304759887\n",
      "0.001373134297666265\n",
      "0.0011192893150834566\n",
      "0.0009726367941802711\n",
      "0.0011592039541892744\n",
      "0.0010074626640486184\n",
      "0.0026442785478609414\n",
      "0.0014771573273890514\n",
      "0.0018034825467536995\n",
      "0.0016069651382108825\n",
      "0.0015348258363407346\n",
      "0.0014626865344705866\n",
      "0.0009720811965464032\n",
      "0.0012014925104579818\n",
      "0.001064676593118046\n",
      "0.0018930347835580212\n",
      "0.001032338285383152\n",
      "0.0013274111378427387\n",
      "################################\n",
      "Episode: 100, score: 0.001327\n",
      "0.0013274111378427387\n",
      "0.0014552238480702265\n",
      "0.001064676593118046\n",
      "0.0018482586651558603\n",
      "0.001497512404338934\n",
      "0.0010355329717778917\n",
      "0.0019104477184921948\n",
      "0.0012711442501946765\n",
      "0.001629353197411963\n",
      "0.0006890547109665859\n",
      "0.0012461928655464335\n",
      "0.0014676616587374935\n",
      "0.0016268656352785096\n",
      "0.0014427860374029596\n",
      "0.0013258706171306509\n",
      "0.0011954314453612427\n",
      "0.0009502487349791907\n",
      "0.001701492499282111\n",
      "0.0016791044400810305\n",
      "0.0011741293269899946\n",
      "0.0017817258485001962\n",
      "################################\n",
      "Episode: 120, score: 0.001782\n",
      "0.0017817258485001962\n",
      "0.0009004974923101231\n",
      "0.0013582089248655447\n",
      "0.0008159203797727082\n",
      "0.0016592039430134036\n",
      "0.0013781725580279294\n",
      "0.0015273631499403745\n",
      "0.0015920397654101623\n",
      "0.001557213895541815\n",
      "0.0014552238480702265\n",
      "0.0015329948895927614\n",
      "0.001425373102468786\n",
      "0.0011592039541892744\n",
      "0.0013781094219331718\n",
      "0.0014378109131360528\n",
      "0.00154060910262054\n",
      "0.0012985074336626637\n",
      "0.0014378109131360528\n",
      "0.0019328357776932752\n",
      "0.001333333303531011\n",
      "0.002131979647778012\n",
      "################################\n",
      "Episode: 140, score: 0.002132\n",
      "0.002131979647778012\n",
      "0.003203980027887952\n",
      "0.0019726367718285293\n",
      "0.002858208891337932\n",
      "0.0024552238257184848\n",
      "0.0016852791501483337\n",
      "0.0012189054453921556\n",
      "0.0013830845462000786\n",
      "0.0021442785590368124\n",
      "0.001044776096050419\n",
      "0.0012208121554538381\n",
      "0.0013905472326004387\n",
      "0.001820895481687873\n",
      "0.0014726367830044\n",
      "0.0024402984529177645\n",
      "0.001413705552157563\n",
      "0.002373134275314523\n",
      "0.0017686566768853522\n",
      "0.0015149253392731075\n",
      "0.0015995024518105224\n",
      "0.001474619256379792\n",
      "################################\n",
      "Episode: 160, score: 0.001475\n",
      "0.001474619256379792\n",
      "0.0009378109243119237\n",
      "0.002062189008632851\n",
      "0.002121890499835732\n",
      "0.0015199004635400143\n",
      "0.002492385731092867\n",
      "0.0011940298240576217\n",
      "0.0013308457413975575\n",
      "0.0014477611616698664\n",
      "0.0013656716112659048\n",
      "0.0014111674811483036\n",
      "0.0017487561798177251\n",
      "0.002129353186236092\n",
      "0.0016840795643479373\n",
      "0.0012910447472623036\n",
      "0.0016142131618890666\n",
      "0.0010970149008529399\n",
      "0.0015223880256734676\n",
      "0.0017363183691504582\n",
      "0.000788557196304721\n",
      "0.0014314720492223797\n",
      "################################\n",
      "Episode: 180, score: 0.001431\n",
      "0.0014314720492223797\n",
      "0.0019676616475616227\n",
      "0.0014278606646022393\n",
      "0.001609452700344336\n",
      "0.0013432835520648244\n",
      "0.001243654794537174\n",
      "0.0013084576821964771\n",
      "0.0013482586763317313\n",
      "0.0013955223568673453\n",
      "0.0010721392795184062\n",
      "0.002007614168324295\n",
      "0.001412935291801519\n",
      "0.0013582089248655447\n",
      "0.0012462686288601426\n",
      "0.0006616915274985987\n",
      "0.001342639563898296\n",
      "0.0013830845462000786\n",
      "0.0017114427478159245\n",
      "0.0016641790672803102\n",
      "0.0014875621558051203\n",
      "0.0014822334694075704\n",
      "################################\n",
      "Episode: 200, score: 0.001482\n",
      "0.0014822334694075704\n",
      "0.0018059701088871528\n",
      "0.0013781094219331718\n",
      "0.0014527362859367732\n",
      "0.001997512393163063\n",
      "0.0008045685099352738\n",
      "0.0009228855515112035\n",
      "0.001425373102468786\n",
      "0.00091293530297739\n",
      "0.0020323382630314103\n",
      "0.0015431471736297995\n",
      "0.0013855721083335319\n",
      "0.0007412935157691069\n",
      "0.001793532298219886\n",
      "0.0011741293269899946\n",
      "0.0011218273860927161\n",
      "0.0012462686288601426\n",
      "0.001820895481687873\n",
      "0.0018507462272893138\n",
      "0.0015398009606076414\n",
      "0.0023426395415465542\n",
      "################################\n",
      "Episode: 220, score: 0.002343\n",
      "0.0023426395415465542\n",
      "0.0012935323093957569\n",
      "0.001504975090739294\n",
      "0.0014925372800720271\n",
      "0.0011144278357871136\n",
      "0.001451776617296456\n",
      "0.0013781094219331718\n",
      "0.0018109452331540596\n",
      "0.0011343283328547407\n",
      "0.0013358208656644643\n",
      "0.0018883248308890967\n",
      "0.0014104477296680658\n",
      "0.0010422885339169657\n",
      "0.0019278606534263683\n",
      "0.0015497512091414548\n",
      "0.00137563448701867\n",
      "0.0022686566657094814\n",
      "0.0013905472326004387\n",
      "0.0016741293158141239\n",
      "0.0008955223680432163\n",
      "0.002063451730528005\n",
      "################################\n",
      "Episode: 240, score: 0.002063\n",
      "0.002063451730528005\n",
      "0.002064676570766304\n",
      "0.001694029812881751\n",
      "0.0014552238480702265\n",
      "0.001504975090739294\n",
      "0.0018248730556576082\n",
      "0.0017114427478159245\n",
      "0.0014601989723371333\n",
      "0.0008930348059097629\n",
      "0.0013258706171306509\n",
      "0.0016192893039075856\n",
      "0.0019527362747609022\n",
      "0.0015870646411432555\n",
      "0.0011990049483245285\n",
      "0.001950248712627449\n",
      "0.0014898476824353492\n",
      "0.001661691505146857\n",
      "0.0017810944875526191\n",
      "0.0018084576710206063\n",
      "0.0018681591622234873\n",
      "0.001545685244639059\n",
      "################################\n",
      "Episode: 260, score: 0.001546\n",
      "0.001545685244639059\n",
      "0.0014228855403353325\n",
      "0.0019850745824957963\n",
      "0.001509950215006201\n",
      "0.002432835766517404\n",
      "0.001715736002259448\n",
      "0.002101990002768105\n",
      "0.002601989991592234\n",
      "0.0021940298017058798\n",
      "0.0019601989611612624\n",
      "0.001748730925379822\n",
      "0.0009154228651108434\n",
      "0.0015298507120738278\n",
      "0.0014104477296680658\n",
      "0.0009054726165770298\n",
      "0.0014949238244538682\n",
      "0.00183333329235514\n",
      "0.001912935280625648\n",
      "0.002069651695033211\n",
      "0.0015422885227410947\n",
      "0.0021497461448428293\n",
      "################################\n",
      "Episode: 280, score: 0.002150\n",
      "0.0021497461448428293\n",
      "0.001793532298219886\n",
      "0.0012786069365950366\n",
      "0.0014900497179385738\n",
      "0.00146517409660404\n",
      "0.0017233502152872266\n",
      "0.0015721392683425352\n",
      "0.0027910447137346906\n",
      "0.0013482586763317313\n",
      "0.0017164178720828313\n",
      "0.0014289339782131202\n",
      "0.0016741293158141239\n",
      "0.0019203979670260082\n",
      "0.0020472636358321306\n",
      "0.0013557213627320914\n",
      "0.0022436547721854322\n",
      "0.0011641790784561812\n",
      "0.0012114427589917955\n",
      "0.001569651706209082\n",
      "0.0018432835408889537\n",
      "0.0025888324294447295\n",
      "################################\n",
      "Episode: 300, score: 0.002589\n",
      "0.0025888324294447295\n",
      "0.0012761193744615833\n",
      "0.0019601989611612624\n",
      "0.0015995024518105224\n",
      "0.001024875598982792\n",
      "0.0021548222868613483\n",
      "0.002477611884919565\n",
      "0.002358208902513803\n",
      "0.0016467661323461367\n",
      "0.0013358208656644643\n",
      "0.0018629441207965013\n",
      "0.0016343283216788697\n",
      "0.002509950192654459\n",
      "0.0015323382742072813\n",
      "0.001216417883258702\n",
      "0.0014213197651853416\n",
      "0.002417910393716684\n",
      "0.002579601932391154\n",
      "0.0016393034459457766\n",
      "0.002057213884365944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0021370557897965317\n",
      "################################\n",
      "Episode: 320, score: 0.002137\n",
      "0.0021370557897965317\n",
      "0.0017960198603533394\n",
      "0.0021641790561044393\n",
      "0.0008134328176392548\n",
      "0.0023905472102486967\n",
      "0.0016243654459261046\n",
      "0.0016119402624777894\n",
      "0.002176616866771706\n",
      "0.0021716417425047996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-32944bb4fafd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# uncomment to utilize your own clipped function!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mclipped_surrogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;31m#L.requires_grad_() # I needed to do that to compute something but maybe that means that there is a bug.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-150-a06aa64dd1a1>\u001b[0m in \u001b[0;36mclipped_surrogate\u001b[1;34m(device, policy, old_probs, actions, states, rewards, batch_size, discount, epsilon, beta)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Navigation2\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Navigation2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################################### MAdddIN_CODE #################################################\n",
    "# training loop max iterations\n",
    "episode = 3000\n",
    "\n",
    "# widget bar to display progress\n",
    "#!pip install progressbar\n",
    "#import progressbar as pb\n",
    "#widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "#timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "tmax = 200\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "SGD_epoch = 4\n",
    "batch_size = 200\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    states, actions, rewards,prob = collect_trajectories(env,env_info, policy,device,tmax)\n",
    "    total_rewards = np.mean(rewards)\n",
    "    print(total_rewards)\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        clipped_surrogate(device,policy,prob,actions, states, rewards,batch_size, epsilon=epsilon, beta=beta)\n",
    "        #L.requires_grad_() # I needed to do that to compute something but maybe that means that there is a bug.\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    #epsilon*=.9999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    #beta*=.9999\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"################################\")\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    #timer.update(e+1)\n",
    "    \n",
    "#timer.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Search parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 150\n",
    "tmax = 200\n",
    "SGD_epoch_tab = np.array([4,6,10])\n",
    "lr_tab = np.array([1e-4,5e-4,1e-3])\n",
    "discount_rate_tab = np.array([.99,.95,.90])\n",
    "epsilon_tab = np.array([0.5,0.3,0.1])\n",
    "beta_tab = np.array([.01,.03,.06])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000924\n",
      "0.000923857847370472\n",
      "################################\n",
      "Episode: 40, score: 0.000893\n",
      "0.0008934009952593576\n",
      "################################\n",
      "Episode: 60, score: 0.000997\n",
      "0.0009974619066389987\n",
      "################################\n",
      "Episode: 80, score: 0.001617\n",
      "0.001616751232898326\n",
      "################################\n",
      "Episode: 100, score: 0.001358\n",
      "0.001357867989953853\n",
      "################################\n",
      "Episode: 120, score: 0.002084\n",
      "0.0020837562986020814\n",
      "################################\n",
      "Episode: 140, score: 0.002299\n",
      "0.002299492334389142\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000152\n",
      "0.00015228426055557232\n",
      "################################\n",
      "Episode: 40, score: 0.000635\n",
      "0.0006345177523148847\n",
      "################################\n",
      "Episode: 60, score: 0.000607\n",
      "0.0006065989712130297\n",
      "################################\n",
      "Episode: 80, score: 0.000838\n",
      "0.0008375634330556477\n",
      "################################\n",
      "Episode: 100, score: 0.001332\n",
      "0.0013324872798612577\n",
      "################################\n",
      "Episode: 120, score: 0.001129\n",
      "0.0011294415991204947\n",
      "################################\n",
      "Episode: 140, score: 0.000746\n",
      "0.0007461928767223044\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000612\n",
      "0.0006116751132315488\n",
      "################################\n",
      "Episode: 40, score: 0.001234\n",
      "0.0012335025105001357\n",
      "################################\n",
      "Episode: 60, score: 0.001416\n",
      "0.0014162436231668226\n",
      "################################\n",
      "Episode: 80, score: 0.000952\n",
      "0.000951776628472327\n",
      "################################\n",
      "Episode: 100, score: 0.001246\n",
      "0.0012461928655464335\n",
      "################################\n",
      "Episode: 120, score: 0.001178\n",
      "0.001177664948296426\n",
      "################################\n",
      "Episode: 140, score: 0.001779\n",
      "0.0017791877774909366\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000769\n",
      "0.0007690355158056402\n",
      "################################\n",
      "Episode: 40, score: 0.000657\n",
      "0.0006573603913982205\n",
      "################################\n",
      "Episode: 60, score: 0.000305\n",
      "0.00030456852111114465\n",
      "################################\n",
      "Episode: 80, score: 0.000739\n",
      "0.0007385786636945257\n",
      "################################\n",
      "Episode: 100, score: 0.000977\n",
      "0.0009771573385649224\n",
      "################################\n",
      "Episode: 120, score: 0.001632\n",
      "0.0016319796589538834\n",
      "################################\n",
      "Episode: 140, score: 0.001000\n",
      "0.0009999999776482583\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.001053\n",
      "0.0010532994688427085\n",
      "################################\n",
      "Episode: 40, score: 0.000779\n",
      "0.0007791877998426784\n",
      "################################\n",
      "Episode: 60, score: 0.001223\n",
      "0.0012233502264630976\n",
      "################################\n",
      "Episode: 80, score: 0.001442\n",
      "0.001441624333259418\n",
      "################################\n",
      "Episode: 100, score: 0.001089\n",
      "0.001088832462972342\n",
      "################################\n",
      "Episode: 120, score: 0.001203\n",
      "0.0012030456583890213\n",
      "################################\n",
      "Episode: 140, score: 0.001175\n",
      "0.0011751268772871664\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000914\n",
      "0.0009137055633334339\n",
      "################################\n",
      "Episode: 40, score: 0.001107\n",
      "0.0011065989600371588\n",
      "################################\n",
      "Episode: 60, score: 0.001769\n",
      "0.0017690354934538984\n",
      "################################\n",
      "Episode: 80, score: 0.001901\n",
      "0.0019010151859353945\n",
      "################################\n",
      "Episode: 100, score: 0.001409\n",
      "0.0014086294101390438\n",
      "################################\n",
      "Episode: 120, score: 0.001777\n",
      "0.001776649706481677\n",
      "################################\n",
      "Episode: 140, score: 0.001383\n",
      "0.0013832487000464484\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000386\n",
      "0.00038578679340744987\n",
      "################################\n",
      "Episode: 40, score: 0.000414\n",
      "0.0004137055745093048\n",
      "################################\n",
      "Episode: 60, score: 0.000746\n",
      "0.0007461928767223044\n",
      "################################\n",
      "Episode: 80, score: 0.000546\n",
      "0.0005456852669908009\n",
      "################################\n",
      "Episode: 100, score: 0.001114\n",
      "0.0011142131730649373\n",
      "################################\n",
      "Episode: 120, score: 0.000599\n",
      "0.0005989847581852511\n",
      "################################\n",
      "Episode: 140, score: 0.000850\n",
      "0.0008502537881019454\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000442\n",
      "0.0004416243556111597\n",
      "################################\n",
      "Episode: 40, score: 0.001703\n",
      "0.0017030456472131503\n",
      "################################\n",
      "Episode: 60, score: 0.001528\n",
      "0.0015279187475742422\n",
      "################################\n",
      "Episode: 80, score: 0.001307\n",
      "0.0013071065697686623\n",
      "################################\n",
      "Episode: 100, score: 0.001850\n",
      "0.0018502537657502035\n",
      "################################\n",
      "Episode: 120, score: 0.002467\n",
      "0.0024670050210002714\n",
      "################################\n",
      "Episode: 140, score: 0.001802\n",
      "0.0018020304165742725\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000345\n",
      "0.00034517765725929723\n",
      "################################\n",
      "Episode: 40, score: 0.000343\n",
      "0.0003426395862500377\n",
      "################################\n",
      "Episode: 60, score: 0.000447\n",
      "0.0004467004976296788\n",
      "################################\n",
      "Episode: 80, score: 0.000414\n",
      "0.0004137055745093048\n",
      "################################\n",
      "Episode: 100, score: 0.000622\n",
      "0.000621827397268587\n",
      "################################\n",
      "Episode: 120, score: 0.000997\n",
      "0.0009974619066389987\n",
      "################################\n",
      "Episode: 140, score: 0.000825\n",
      "0.00082487307800935\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.001193\n",
      "0.0011928933743519832\n",
      "################################\n",
      "Episode: 40, score: 0.001604\n",
      "0.0016040608778520283\n",
      "################################\n",
      "Episode: 60, score: 0.001558\n",
      "0.0015583755996853568\n",
      "################################\n",
      "Episode: 80, score: 0.001921\n",
      "0.0019213197540094706\n",
      "################################\n",
      "Episode: 100, score: 0.001734\n",
      "0.001733502499324265\n",
      "################################\n",
      "Episode: 120, score: 0.001241\n",
      "0.0012411167235279145\n",
      "################################\n",
      "Episode: 140, score: 0.001221\n",
      "0.0012208121554538381\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000551\n",
      "0.0005507614090093199\n",
      "################################\n",
      "Episode: 40, score: 0.001239\n",
      "0.001238578652518655\n",
      "################################\n",
      "Episode: 60, score: 0.000673\n",
      "0.0006725888174537777\n",
      "################################\n",
      "Episode: 80, score: 0.001018\n",
      "0.001017766474713075\n",
      "################################\n",
      "Episode: 100, score: 0.001114\n",
      "0.0011142131730649373\n",
      "################################\n",
      "Episode: 120, score: 0.001266\n",
      "0.0012664974336205098\n",
      "################################\n",
      "Episode: 140, score: 0.001396\n",
      "0.0013959390550927462\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000503\n",
      "0.0005025380598333886\n",
      "################################\n",
      "Episode: 40, score: 0.000975\n",
      "0.0009746192675556629\n",
      "################################\n",
      "Episode: 60, score: 0.001343\n",
      "0.001342639563898296\n",
      "################################\n",
      "Episode: 80, score: 0.001322\n",
      "0.0013223349958242196\n",
      "################################\n",
      "Episode: 100, score: 0.001414\n",
      "0.001413705552157563\n",
      "################################\n",
      "Episode: 120, score: 0.000967\n",
      "0.0009670050545278842\n",
      "################################\n",
      "Episode: 140, score: 0.001051\n",
      "0.001050761397833449\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000419\n",
      "0.00041878171652782385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Episode: 40, score: 0.000898\n",
      "0.0008984771372778766\n",
      "################################\n",
      "Episode: 60, score: 0.000442\n",
      "0.0004416243556111597\n",
      "################################\n",
      "Episode: 80, score: 0.001162\n",
      "0.0011624365222408686\n",
      "################################\n",
      "Episode: 100, score: 0.000693\n",
      "0.0006928933855278541\n",
      "################################\n",
      "Episode: 120, score: 0.000886\n",
      "0.0008857867822315789\n",
      "################################\n",
      "Episode: 140, score: 0.000571\n",
      "0.0005710659770833962\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000447\n",
      "0.0004467004976296788\n",
      "################################\n",
      "Episode: 40, score: 0.000685\n",
      "0.0006852791725000754\n",
      "################################\n",
      "Episode: 60, score: 0.001467\n",
      "0.0014670050433520133\n",
      "################################\n",
      "Episode: 80, score: 0.001008\n",
      "0.0010076141906760368\n",
      "################################\n",
      "Episode: 100, score: 0.001061\n",
      "0.001060913681870487\n",
      "################################\n",
      "Episode: 120, score: 0.001536\n",
      "0.001535532960602021\n",
      "################################\n",
      "Episode: 140, score: 0.001155\n",
      "0.00115482230921309\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000421\n",
      "0.0004213197875370834\n",
      "################################\n",
      "Episode: 40, score: 0.000787\n",
      "0.0007868020128704569\n",
      "################################\n",
      "Episode: 60, score: 0.000492\n",
      "0.0004923857757963505\n",
      "################################\n",
      "Episode: 80, score: 0.000650\n",
      "0.0006497461783704419\n",
      "################################\n",
      "Episode: 100, score: 0.001241\n",
      "0.0012411167235279145\n",
      "################################\n",
      "Episode: 120, score: 0.001485\n",
      "0.0014847715404168302\n",
      "################################\n",
      "Episode: 140, score: 0.001490\n",
      "0.0014898476824353492\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000607\n",
      "0.0006065989712130297\n",
      "################################\n",
      "Episode: 40, score: 0.000853\n",
      "0.000852791859111205\n",
      "################################\n",
      "Episode: 60, score: 0.001668\n",
      "0.0016675126530835169\n",
      "################################\n",
      "Episode: 80, score: 0.001553\n",
      "0.0015532994576668375\n",
      "################################\n",
      "Episode: 100, score: 0.000909\n",
      "0.0009086294213149148\n",
      "################################\n",
      "Episode: 120, score: 0.001881\n",
      "0.0018807106178613182\n",
      "################################\n",
      "Episode: 140, score: 0.001277\n",
      "0.001276649717657548\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000662\n",
      "0.0006624365334167396\n",
      "################################\n",
      "Episode: 40, score: 0.000883\n",
      "0.0008832487112223194\n",
      "################################\n",
      "Episode: 60, score: 0.001685\n",
      "0.0016852791501483337\n",
      "################################\n",
      "Episode: 80, score: 0.001137\n",
      "0.0011370558121482732\n",
      "################################\n",
      "Episode: 100, score: 0.001503\n",
      "0.0015025380374816468\n",
      "################################\n",
      "Episode: 120, score: 0.001929\n",
      "0.0019289339670372494\n",
      "################################\n",
      "Episode: 140, score: 0.001155\n",
      "0.00115482230921309\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000576\n",
      "0.0005761421191019153\n",
      "################################\n",
      "Episode: 40, score: 0.000916\n",
      "0.0009162436343426935\n",
      "################################\n",
      "Episode: 60, score: 0.000947\n",
      "0.0009467004864538079\n",
      "################################\n",
      "Episode: 80, score: 0.000995\n",
      "0.0009949238356297392\n",
      "################################\n",
      "Episode: 100, score: 0.001934\n",
      "0.0019340101090557684\n",
      "################################\n",
      "Episode: 120, score: 0.001041\n",
      "0.0010406091137964107\n",
      "################################\n",
      "Episode: 140, score: 0.001165\n",
      "0.0011649745932501283\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000739\n",
      "0.0007385786636945257\n",
      "################################\n",
      "Episode: 40, score: 0.000482\n",
      "0.00048223349175931235\n",
      "################################\n",
      "Episode: 60, score: 0.000581\n",
      "0.0005812182611204343\n",
      "################################\n",
      "Episode: 80, score: 0.001124\n",
      "0.0011243654571019756\n",
      "################################\n",
      "Episode: 100, score: 0.000690\n",
      "0.0006903553145185945\n",
      "################################\n",
      "Episode: 120, score: 0.001018\n",
      "0.001017766474713075\n",
      "################################\n",
      "Episode: 140, score: 0.000617\n",
      "0.0006167512552500678\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000652\n",
      "0.0006522842493797014\n",
      "################################\n",
      "Episode: 40, score: 0.000645\n",
      "0.0006446700363519228\n",
      "################################\n",
      "Episode: 60, score: 0.001046\n",
      "0.00104568525581493\n",
      "################################\n",
      "Episode: 80, score: 0.000695\n",
      "0.0006954314565371136\n",
      "################################\n",
      "Episode: 100, score: 0.001464\n",
      "0.0014644669723427538\n",
      "################################\n",
      "Episode: 120, score: 0.001289\n",
      "0.0012893400727038457\n",
      "################################\n",
      "Episode: 140, score: 0.000706\n",
      "0.0007055837405741518\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000723\n",
      "0.0007233502376389685\n",
      "################################\n",
      "Episode: 40, score: 0.001312\n",
      "0.0013121827117871816\n",
      "################################\n",
      "Episode: 60, score: 0.000563\n",
      "0.0005634517640556176\n",
      "################################\n",
      "Episode: 80, score: 0.001373\n",
      "0.0013730964160094104\n",
      "################################\n",
      "Episode: 100, score: 0.001467\n",
      "0.0014670050433520133\n",
      "################################\n",
      "Episode: 120, score: 0.001381\n",
      "0.001380710629037189\n",
      "################################\n",
      "Episode: 140, score: 0.001454\n",
      "0.0014543146883057155\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000594\n",
      "0.000593908616166732\n",
      "################################\n",
      "Episode: 40, score: 0.000820\n",
      "0.000819796935990831\n",
      "################################\n",
      "Episode: 60, score: 0.000876\n",
      "0.0008756344981945408\n",
      "################################\n",
      "Episode: 80, score: 0.000553\n",
      "0.0005532994800185794\n",
      "################################\n",
      "Episode: 100, score: 0.000921\n",
      "0.0009213197763612125\n",
      "################################\n",
      "Episode: 120, score: 0.000713\n",
      "0.0007131979536019303\n",
      "################################\n",
      "Episode: 140, score: 0.000827\n",
      "0.0008274111490186096\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000977\n",
      "0.0009771573385649224\n",
      "################################\n",
      "Episode: 40, score: 0.000536\n",
      "0.0005355329829537627\n",
      "################################\n",
      "Episode: 60, score: 0.000784\n",
      "0.0007842639418611974\n",
      "################################\n",
      "Episode: 80, score: 0.000693\n",
      "0.0006928933855278541\n",
      "################################\n",
      "Episode: 100, score: 0.000693\n",
      "0.0006928933855278541\n",
      "################################\n",
      "Episode: 120, score: 0.000832\n",
      "0.0008324872910371287\n",
      "################################\n",
      "Episode: 140, score: 0.000759\n",
      "0.000758883231768602\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000746\n",
      "0.0007461928767223044\n",
      "################################\n",
      "Episode: 40, score: 0.000876\n",
      "0.0008756344981945408\n",
      "################################\n",
      "Episode: 60, score: 0.000668\n",
      "0.0006675126754352586\n",
      "################################\n",
      "Episode: 80, score: 0.000853\n",
      "0.000852791859111205\n",
      "################################\n",
      "Episode: 100, score: 0.000660\n",
      "0.00065989846240748\n",
      "################################\n",
      "Episode: 120, score: 0.001241\n",
      "0.0012411167235279145\n",
      "################################\n",
      "Episode: 140, score: 0.000835\n",
      "0.0008350253620463882\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000000\n",
      "0.0\n",
      "################################\n",
      "Episode: 40, score: 0.000259\n",
      "0.0002588832429444729\n",
      "################################\n",
      "Episode: 60, score: 0.000640\n",
      "0.0006395938943334037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Episode: 80, score: 0.001525\n",
      "0.0015253806765649826\n",
      "################################\n",
      "Episode: 100, score: 0.000655\n",
      "0.0006548223203889609\n",
      "################################\n",
      "Episode: 120, score: 0.001464\n",
      "0.0014644669723427538\n",
      "################################\n",
      "Episode: 140, score: 0.001761\n",
      "0.0017614212804261198\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000371\n",
      "0.0003705583673518926\n",
      "################################\n",
      "Episode: 40, score: 0.000698\n",
      "0.0006979695275463731\n",
      "################################\n",
      "Episode: 60, score: 0.000558\n",
      "0.0005583756220370985\n",
      "################################\n",
      "Episode: 80, score: 0.000404\n",
      "0.00040355329047226665\n",
      "################################\n",
      "Episode: 100, score: 0.001145\n",
      "0.001144670025176052\n",
      "################################\n",
      "Episode: 120, score: 0.000860\n",
      "0.0008604060721389836\n",
      "################################\n",
      "Episode: 140, score: 0.001170\n",
      "0.0011700507352686474\n",
      "{'SGD_epoch': 4, 'lr': 0.0001, 'discount_rate': 0.9, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000530\n",
      "0.0005304568409352435\n",
      "################################\n",
      "Episode: 40, score: 0.000569\n",
      "0.0005685279060741366\n",
      "################################\n",
      "Episode: 60, score: 0.000741\n",
      "0.0007411167347037852\n",
      "################################\n",
      "Episode: 80, score: 0.001015\n",
      "0.0010152284037038153\n",
      "################################\n",
      "Episode: 100, score: 0.001536\n",
      "0.001535532960602021\n",
      "################################\n",
      "Episode: 120, score: 0.000881\n",
      "0.0008807106402130599\n",
      "################################\n",
      "Episode: 140, score: 0.001513\n",
      "0.001512690321518685\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.001274\n",
      "0.0012741116466482884\n",
      "################################\n",
      "Episode: 40, score: 0.001345\n",
      "0.0013451776349075555\n",
      "################################\n",
      "Episode: 60, score: 0.001378\n",
      "0.0013781725580279294\n",
      "################################\n",
      "Episode: 80, score: 0.001693\n",
      "0.0016928933631761122\n",
      "################################\n",
      "Episode: 100, score: 0.001627\n",
      "0.0016269035169353642\n",
      "################################\n",
      "Episode: 120, score: 0.001746\n",
      "0.0017461928543705625\n",
      "################################\n",
      "Episode: 140, score: 0.001548\n",
      "0.0015482233156483185\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000655\n",
      "0.0006548223203889609\n",
      "################################\n",
      "Episode: 40, score: 0.001211\n",
      "0.0012106598714167998\n",
      "################################\n",
      "Episode: 60, score: 0.001662\n",
      "0.0016624365110649978\n",
      "################################\n",
      "Episode: 80, score: 0.002297\n",
      "0.0022969542633798825\n",
      "################################\n",
      "Episode: 100, score: 0.002289\n",
      "0.002289340050352104\n",
      "################################\n",
      "Episode: 120, score: 0.001972\n",
      "0.0019720811741946614\n",
      "################################\n",
      "Episode: 140, score: 0.002355\n",
      "0.002355329896592852\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000173\n",
      "0.00017258882862964862\n",
      "################################\n",
      "Episode: 40, score: 0.000985\n",
      "0.000984771551592701\n",
      "################################\n",
      "Episode: 60, score: 0.001165\n",
      "0.0011649745932501283\n",
      "################################\n",
      "Episode: 80, score: 0.001284\n",
      "0.0012842639306853264\n",
      "################################\n",
      "Episode: 100, score: 0.000802\n",
      "0.0008020304389260141\n",
      "################################\n",
      "Episode: 120, score: 0.000992\n",
      "0.0009923857646204795\n",
      "################################\n",
      "Episode: 140, score: 0.000541\n",
      "0.0005406091249722817\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000500\n",
      "0.0004999999888241291\n",
      "################################\n",
      "Episode: 40, score: 0.000627\n",
      "0.000626903539287106\n",
      "################################\n",
      "Episode: 60, score: 0.001386\n",
      "0.0013857867710557082\n",
      "################################\n",
      "Episode: 80, score: 0.001563\n",
      "0.0015634517417038758\n",
      "################################\n",
      "Episode: 100, score: 0.002442\n",
      "0.0024416243109076762\n",
      "################################\n",
      "Episode: 120, score: 0.001830\n",
      "0.0018299491976761274\n",
      "################################\n",
      "Episode: 140, score: 0.001360\n",
      "0.0013604060609631128\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.001538\n",
      "0.0015380710316112804\n",
      "################################\n",
      "Episode: 40, score: 0.002609\n",
      "0.0026091369975188056\n",
      "################################\n",
      "Episode: 60, score: 0.002718\n",
      "0.0027182740509169656\n",
      "################################\n",
      "Episode: 80, score: 0.003033\n",
      "0.0030329948560651487\n",
      "################################\n",
      "Episode: 100, score: 0.003178\n",
      "0.0031776649035929424\n",
      "################################\n",
      "Episode: 120, score: 0.004954\n",
      "0.00495431461007462\n",
      "################################\n",
      "Episode: 140, score: 0.003695\n",
      "0.003695431389481888\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.001594\n",
      "0.0015939085938149902\n",
      "################################\n",
      "Episode: 40, score: 0.001190\n",
      "0.0011903553033427237\n",
      "################################\n",
      "Episode: 60, score: 0.000952\n",
      "0.000951776628472327\n",
      "################################\n",
      "Episode: 80, score: 0.001777\n",
      "0.001776649706481677\n",
      "################################\n",
      "Episode: 100, score: 0.001218\n",
      "0.0012182740844445786\n",
      "################################\n",
      "Episode: 120, score: 0.002160\n",
      "0.0021598984288798673\n",
      "################################\n",
      "Episode: 140, score: 0.001574\n",
      "0.0015736040257409139\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000995\n",
      "0.0009949238356297392\n",
      "################################\n",
      "Episode: 40, score: 0.001201\n",
      "0.0012005075873797618\n",
      "################################\n",
      "Episode: 60, score: 0.001728\n",
      "0.001728426357305746\n",
      "################################\n",
      "Episode: 80, score: 0.001236\n",
      "0.0012360405815093952\n",
      "################################\n",
      "Episode: 100, score: 0.001330\n",
      "0.0013299492088519982\n",
      "################################\n",
      "Episode: 120, score: 0.002711\n",
      "0.002710659837889187\n",
      "################################\n",
      "Episode: 140, score: 0.003756\n",
      "0.0037563450937041173\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.001426\n",
      "0.0014263959072038607\n",
      "################################\n",
      "Episode: 40, score: 0.001246\n",
      "0.0012461928655464335\n",
      "################################\n",
      "Episode: 60, score: 0.001873\n",
      "0.0018730964048335394\n",
      "################################\n",
      "Episode: 80, score: 0.002061\n",
      "0.0020609136595187453\n",
      "################################\n",
      "Episode: 100, score: 0.002840\n",
      "0.0028401014593614237\n",
      "################################\n",
      "Episode: 120, score: 0.002076\n",
      "0.0020761420855743024\n",
      "################################\n",
      "Episode: 140, score: 0.003345\n",
      "0.0033451775902040718\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.001637\n",
      "0.0016370558009724024\n",
      "################################\n",
      "Episode: 40, score: 0.002043\n",
      "0.0020431471624539287\n",
      "################################\n",
      "Episode: 60, score: 0.002470\n",
      "0.002469543092009531\n",
      "################################\n",
      "Episode: 80, score: 0.002015\n",
      "0.0020152283813520736\n",
      "################################\n",
      "Episode: 100, score: 0.002421\n",
      "0.0024213197428335997\n",
      "################################\n",
      "Episode: 120, score: 0.002787\n",
      "0.0027868019681669734\n",
      "################################\n",
      "Episode: 140, score: 0.003036\n",
      "0.003035532927074408\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000883\n",
      "0.0008832487112223194\n",
      "################################\n",
      "Episode: 40, score: 0.000802\n",
      "0.0008020304389260141\n",
      "################################\n",
      "Episode: 60, score: 0.000678\n",
      "0.0006776649594722968\n",
      "################################\n",
      "Episode: 80, score: 0.000888\n",
      "0.0008883248532408385\n",
      "################################\n",
      "Episode: 100, score: 0.001335\n",
      "0.0013350253508705172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Episode: 120, score: 0.002063\n",
      "0.002063451730528005\n",
      "################################\n",
      "Episode: 140, score: 0.001650\n",
      "0.0016497461560187\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000848\n",
      "0.0008477157170926859\n",
      "################################\n",
      "Episode: 40, score: 0.001023\n",
      "0.0010228426167315941\n",
      "################################\n",
      "Episode: 60, score: 0.001487\n",
      "0.0014873096114260897\n",
      "################################\n",
      "Episode: 80, score: 0.001843\n",
      "0.001842639552722425\n",
      "################################\n",
      "Episode: 100, score: 0.001668\n",
      "0.0016675126530835169\n",
      "################################\n",
      "Episode: 120, score: 0.002657\n",
      "0.002657360346694737\n",
      "################################\n",
      "Episode: 140, score: 0.001439\n",
      "0.0014390862622501584\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000906\n",
      "0.0009060913503056553\n",
      "################################\n",
      "Episode: 40, score: 0.001069\n",
      "0.0010685278948982658\n",
      "################################\n",
      "Episode: 60, score: 0.000739\n",
      "0.0007385786636945257\n",
      "################################\n",
      "Episode: 80, score: 0.001096\n",
      "0.0010964466760001207\n",
      "################################\n",
      "Episode: 100, score: 0.000317\n",
      "0.00031725887615744234\n",
      "################################\n",
      "Episode: 120, score: 0.000784\n",
      "0.0007842639418611974\n",
      "################################\n",
      "Episode: 140, score: 0.000751\n",
      "0.0007512690187408234\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000825\n",
      "0.00082487307800935\n",
      "################################\n",
      "Episode: 40, score: 0.001061\n",
      "0.001060913681870487\n",
      "################################\n",
      "Episode: 60, score: 0.000556\n",
      "0.0005558375510278389\n",
      "################################\n",
      "Episode: 80, score: 0.001294\n",
      "0.0012944162147223647\n",
      "################################\n",
      "Episode: 100, score: 0.001447\n",
      "0.001446700475277937\n",
      "################################\n",
      "Episode: 120, score: 0.001558\n",
      "0.0015583755996853568\n",
      "################################\n",
      "Episode: 140, score: 0.001807\n",
      "0.0018071065585927915\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000647\n",
      "0.0006472081073611824\n",
      "################################\n",
      "Episode: 40, score: 0.001657\n",
      "0.0016573603690464788\n",
      "################################\n",
      "Episode: 60, score: 0.001249\n",
      "0.001248730936555693\n",
      "################################\n",
      "Episode: 80, score: 0.000952\n",
      "0.000951776628472327\n",
      "################################\n",
      "Episode: 100, score: 0.001645\n",
      "0.001644670014000181\n",
      "################################\n",
      "Episode: 120, score: 0.002081\n",
      "0.0020812182275928215\n",
      "################################\n",
      "Episode: 140, score: 0.001467\n",
      "0.0014670050433520133\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000614\n",
      "0.0006142131842408083\n",
      "################################\n",
      "Episode: 40, score: 0.001201\n",
      "0.0012005075873797618\n",
      "################################\n",
      "Episode: 60, score: 0.000954\n",
      "0.0009543146994815865\n",
      "################################\n",
      "Episode: 80, score: 0.001241\n",
      "0.0012411167235279145\n",
      "################################\n",
      "Episode: 100, score: 0.000584\n",
      "0.0005837563321296939\n",
      "################################\n",
      "Episode: 120, score: 0.000673\n",
      "0.0006725888174537777\n",
      "################################\n",
      "Episode: 140, score: 0.000744\n",
      "0.0007436548057130448\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000198\n",
      "0.00019796953872224402\n",
      "################################\n",
      "Episode: 40, score: 0.000619\n",
      "0.0006192893262593275\n",
      "################################\n",
      "Episode: 60, score: 0.000716\n",
      "0.0007157360246111898\n",
      "################################\n",
      "Episode: 80, score: 0.000406\n",
      "0.00040609136148152616\n",
      "################################\n",
      "Episode: 100, score: 0.001109\n",
      "0.0011091370310464183\n",
      "################################\n",
      "Episode: 120, score: 0.001107\n",
      "0.0011065989600371588\n",
      "################################\n",
      "Episode: 140, score: 0.001662\n",
      "0.0016624365110649978\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000832\n",
      "0.0008324872910371287\n",
      "################################\n",
      "Episode: 40, score: 0.001371\n",
      "0.0013705583450001509\n",
      "################################\n",
      "Episode: 60, score: 0.001685\n",
      "0.0016852791501483337\n",
      "################################\n",
      "Episode: 80, score: 0.001487\n",
      "0.0014873096114260897\n",
      "################################\n",
      "Episode: 100, score: 0.001779\n",
      "0.0017791877774909366\n",
      "################################\n",
      "Episode: 120, score: 0.001277\n",
      "0.001276649717657548\n",
      "################################\n",
      "Episode: 140, score: 0.002236\n",
      "0.0022360405591576537\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000518\n",
      "0.0005177664858889458\n",
      "################################\n",
      "Episode: 40, score: 0.000977\n",
      "0.0009771573385649224\n",
      "################################\n",
      "Episode: 60, score: 0.001003\n",
      "0.0010025380486575178\n",
      "################################\n",
      "Episode: 80, score: 0.001668\n",
      "0.0016675126530835169\n",
      "################################\n",
      "Episode: 100, score: 0.001404\n",
      "0.0014035532681205248\n",
      "################################\n",
      "Episode: 120, score: 0.001353\n",
      "0.001352791847935334\n",
      "################################\n",
      "Episode: 140, score: 0.002226\n",
      "0.002225888275120615\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000586\n",
      "0.0005862944031389534\n",
      "################################\n",
      "Episode: 40, score: 0.001198\n",
      "0.0011979695163705022\n",
      "################################\n",
      "Episode: 60, score: 0.002079\n",
      "0.002078680156583562\n",
      "################################\n",
      "Episode: 80, score: 0.002137\n",
      "0.0021370557897965317\n",
      "################################\n",
      "Episode: 100, score: 0.001914\n",
      "0.001913705540981692\n",
      "################################\n",
      "Episode: 120, score: 0.001086\n",
      "0.0010862943919630825\n",
      "################################\n",
      "Episode: 140, score: 0.001388\n",
      "0.0013883248420649677\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000906\n",
      "0.0009060913503056553\n",
      "################################\n",
      "Episode: 40, score: 0.001386\n",
      "0.0013857867710557082\n",
      "################################\n",
      "Episode: 60, score: 0.000761\n",
      "0.0007614213027778616\n",
      "################################\n",
      "Episode: 80, score: 0.001086\n",
      "0.0010862943919630825\n",
      "################################\n",
      "Episode: 100, score: 0.001010\n",
      "0.0010101522616852963\n",
      "################################\n",
      "Episode: 120, score: 0.001096\n",
      "0.0010964466760001207\n",
      "################################\n",
      "Episode: 140, score: 0.002104\n",
      "0.0021040608666761575\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000977\n",
      "0.0009771573385649224\n",
      "################################\n",
      "Episode: 40, score: 0.001368\n",
      "0.0013680202739908913\n",
      "################################\n",
      "Episode: 60, score: 0.001208\n",
      "0.0012081218004075403\n",
      "################################\n",
      "Episode: 80, score: 0.000865\n",
      "0.0008654822141575027\n",
      "################################\n",
      "Episode: 100, score: 0.000840\n",
      "0.0008401015040649073\n",
      "################################\n",
      "Episode: 120, score: 0.000815\n",
      "0.0008147207939723118\n",
      "################################\n",
      "Episode: 140, score: 0.000693\n",
      "0.0006928933855278541\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000688\n",
      "0.000687817243509335\n",
      "################################\n",
      "Episode: 40, score: 0.001127\n",
      "0.0011269035281112351\n",
      "################################\n",
      "Episode: 60, score: 0.000799\n",
      "0.0007994923679167546\n",
      "################################\n",
      "Episode: 80, score: 0.001157\n",
      "0.0011573603802223496\n",
      "################################\n",
      "Episode: 100, score: 0.001459\n",
      "0.0014593908303242348\n",
      "################################\n",
      "Episode: 120, score: 0.002244\n",
      "0.0022436547721854322\n",
      "################################\n",
      "Episode: 140, score: 0.000977\n",
      "0.0009771573385649224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000670\n",
      "0.0006700507464445182\n",
      "################################\n",
      "Episode: 40, score: 0.000883\n",
      "0.0008832487112223194\n",
      "################################\n",
      "Episode: 60, score: 0.001084\n",
      "0.001083756320953823\n",
      "################################\n",
      "Episode: 80, score: 0.000987\n",
      "0.0009873096226019605\n",
      "################################\n",
      "Episode: 100, score: 0.001102\n",
      "0.0011015228180186398\n",
      "################################\n",
      "Episode: 120, score: 0.002096\n",
      "0.002096446653648379\n",
      "################################\n",
      "Episode: 140, score: 0.001025\n",
      "0.0010253806877408536\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000992\n",
      "0.0009923857646204795\n",
      "################################\n",
      "Episode: 40, score: 0.001218\n",
      "0.0012182740844445786\n",
      "################################\n",
      "Episode: 60, score: 0.001091\n",
      "0.0010913705339816017\n",
      "################################\n",
      "Episode: 80, score: 0.001183\n",
      "0.001182741090314945\n",
      "################################\n",
      "Episode: 100, score: 0.000860\n",
      "0.0008604060721389836\n",
      "################################\n",
      "Episode: 120, score: 0.001041\n",
      "0.0010406091137964107\n",
      "################################\n",
      "Episode: 140, score: 0.000921\n",
      "0.0009213197763612125\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000386\n",
      "0.00038578679340744987\n",
      "################################\n",
      "Episode: 40, score: 0.001030\n",
      "0.0010304568297593727\n",
      "################################\n",
      "Episode: 60, score: 0.000589\n",
      "0.000588832474148213\n",
      "################################\n",
      "Episode: 80, score: 0.000766\n",
      "0.0007664974447963807\n",
      "################################\n",
      "Episode: 100, score: 0.000642\n",
      "0.0006421319653426632\n",
      "################################\n",
      "Episode: 120, score: 0.000797\n",
      "0.0007969542969074951\n",
      "################################\n",
      "Episode: 140, score: 0.001170\n",
      "0.0011700507352686474\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000817\n",
      "0.0008172588649815715\n",
      "################################\n",
      "Episode: 40, score: 0.000774\n",
      "0.0007741116578241593\n",
      "################################\n",
      "Episode: 60, score: 0.001591\n",
      "0.0015913705228057307\n",
      "################################\n",
      "Episode: 80, score: 0.002043\n",
      "0.0020431471624539287\n",
      "################################\n",
      "Episode: 100, score: 0.002069\n",
      "0.002068527872546524\n",
      "################################\n",
      "Episode: 120, score: 0.002015\n",
      "0.0020152283813520736\n",
      "################################\n",
      "Episode: 140, score: 0.002701\n",
      "0.002700507553852149\n",
      "{'SGD_epoch': 4, 'lr': 0.0005, 'discount_rate': 0.9, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000822\n",
      "0.0008223350070000905\n",
      "################################\n",
      "Episode: 40, score: 0.001561\n",
      "0.0015609136706946163\n",
      "################################\n",
      "Episode: 60, score: 0.001802\n",
      "0.0018020304165742725\n",
      "################################\n",
      "Episode: 80, score: 0.001553\n",
      "0.0015532994576668375\n",
      "################################\n",
      "Episode: 100, score: 0.001497\n",
      "0.0014974618954631278\n",
      "################################\n",
      "Episode: 120, score: 0.001952\n",
      "0.0019517766061205853\n",
      "################################\n",
      "Episode: 140, score: 0.001492\n",
      "0.0014923857534446087\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.001315\n",
      "0.001314720782796441\n",
      "################################\n",
      "Episode: 40, score: 0.000964\n",
      "0.0009644669835186247\n",
      "################################\n",
      "Episode: 60, score: 0.001576\n",
      "0.0015761420967501734\n",
      "################################\n",
      "Episode: 80, score: 0.001695\n",
      "0.0016954314341853717\n",
      "################################\n",
      "Episode: 100, score: 0.002015\n",
      "0.0020152283813520736\n",
      "################################\n",
      "Episode: 120, score: 0.002381\n",
      "0.0023807106066854474\n",
      "################################\n",
      "Episode: 140, score: 0.001388\n",
      "0.0013883248420649677\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000591\n",
      "0.0005913705451574725\n",
      "################################\n",
      "Episode: 40, score: 0.001188\n",
      "0.001187817232333464\n",
      "################################\n",
      "Episode: 60, score: 0.000561\n",
      "0.0005609136930463581\n",
      "################################\n",
      "Episode: 80, score: 0.001708\n",
      "0.0017081217892316695\n",
      "################################\n",
      "Episode: 100, score: 0.001135\n",
      "0.0011345177411390137\n",
      "################################\n",
      "Episode: 120, score: 0.001109\n",
      "0.0011091370310464183\n",
      "################################\n",
      "Episode: 140, score: 0.001447\n",
      "0.001446700475277937\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.001023\n",
      "0.0010228426167315941\n",
      "################################\n",
      "Episode: 40, score: 0.002099\n",
      "0.0020989847246576385\n",
      "################################\n",
      "Episode: 60, score: 0.001739\n",
      "0.001738578641342784\n",
      "################################\n",
      "Episode: 80, score: 0.002513\n",
      "0.002512690299166943\n",
      "################################\n",
      "Episode: 100, score: 0.002827\n",
      "0.002827411104315126\n",
      "################################\n",
      "Episode: 120, score: 0.002302\n",
      "0.0023020304053984015\n",
      "################################\n",
      "Episode: 140, score: 0.001731\n",
      "0.0017309644283150054\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.001152\n",
      "0.0011522842382038305\n",
      "################################\n",
      "Episode: 40, score: 0.000594\n",
      "0.000593908616166732\n",
      "################################\n",
      "Episode: 60, score: 0.000911\n",
      "0.0009111674923241743\n",
      "################################\n",
      "Episode: 80, score: 0.001865\n",
      "0.0018654821918057608\n",
      "################################\n",
      "Episode: 100, score: 0.001751\n",
      "0.0017512689963890815\n",
      "################################\n",
      "Episode: 120, score: 0.001660\n",
      "0.0016598984400557383\n",
      "################################\n",
      "Episode: 140, score: 0.002000\n",
      "0.0019999999552965165\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.001201\n",
      "0.0012005075873797618\n",
      "################################\n",
      "Episode: 40, score: 0.001454\n",
      "0.0014543146883057155\n",
      "################################\n",
      "Episode: 60, score: 0.001959\n",
      "0.001959390819148364\n",
      "################################\n",
      "Episode: 80, score: 0.001850\n",
      "0.0018502537657502035\n",
      "################################\n",
      "Episode: 100, score: 0.001627\n",
      "0.0016269035169353642\n",
      "################################\n",
      "Episode: 120, score: 0.001698\n",
      "0.0016979695051946313\n",
      "################################\n",
      "Episode: 140, score: 0.002728\n",
      "0.002728426334954004\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.001173\n",
      "0.0011725888062779069\n",
      "################################\n",
      "Episode: 40, score: 0.001178\n",
      "0.001177664948296426\n",
      "################################\n",
      "Episode: 60, score: 0.001779\n",
      "0.0017791877774909366\n",
      "################################\n",
      "Episode: 80, score: 0.002132\n",
      "0.002131979647778012\n",
      "################################\n",
      "Episode: 100, score: 0.001881\n",
      "0.0018807106178613182\n",
      "################################\n",
      "Episode: 120, score: 0.002338\n",
      "0.002337563399528035\n",
      "################################\n",
      "Episode: 140, score: 0.001096\n",
      "0.0010964466760001207\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.001053\n",
      "0.0010532994688427085\n",
      "################################\n",
      "Episode: 40, score: 0.002086\n",
      "0.002086294369611341\n",
      "################################\n",
      "Episode: 60, score: 0.002124\n",
      "0.0021243654347502337\n",
      "################################\n",
      "Episode: 80, score: 0.002378\n",
      "0.002378172535676188\n",
      "################################\n",
      "Episode: 100, score: 0.003546\n",
      "0.0035456851999355753\n",
      "################################\n",
      "Episode: 120, score: 0.003135\n",
      "0.00313451769643553\n",
      "################################\n",
      "Episode: 140, score: 0.004216\n",
      "0.004215735946380094\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000845\n",
      "0.0008451776460834264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Episode: 40, score: 0.001723\n",
      "0.0017233502152872266\n",
      "################################\n",
      "Episode: 60, score: 0.002982\n",
      "0.002982233435879958\n",
      "################################\n",
      "Episode: 80, score: 0.002675\n",
      "0.002675126843759554\n",
      "################################\n",
      "Episode: 100, score: 0.004112\n",
      "0.004111675035000453\n",
      "################################\n",
      "Episode: 120, score: 0.002668\n",
      "0.002667512630731775\n",
      "################################\n",
      "Episode: 140, score: 0.004084\n",
      "0.0040837562538985975\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.99, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.001426\n",
      "0.0014263959072038607\n",
      "################################\n",
      "Episode: 40, score: 0.000975\n",
      "0.0009746192675556629\n",
      "################################\n",
      "Episode: 60, score: 0.001619\n",
      "0.0016192893039075856\n",
      "################################\n",
      "Episode: 80, score: 0.004228\n",
      "0.004228426301426391\n",
      "################################\n",
      "Episode: 100, score: 0.004201\n",
      "0.004200507520324536\n",
      "################################\n",
      "Episode: 120, score: 0.004272\n",
      "0.004271573508583803\n",
      "################################\n",
      "Episode: 140, score: 0.004581\n",
      "0.004581218171713467\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.001213\n",
      "0.0012131979424260593\n",
      "################################\n",
      "Episode: 40, score: 0.001563\n",
      "0.0015634517417038758\n",
      "################################\n",
      "Episode: 60, score: 0.001584\n",
      "0.0015837563097779522\n",
      "################################\n",
      "Episode: 80, score: 0.002332\n",
      "0.002332487257509516\n",
      "################################\n",
      "Episode: 100, score: 0.001576\n",
      "0.0015761420967501734\n",
      "################################\n",
      "Episode: 120, score: 0.001784\n",
      "0.0017842639195094557\n",
      "################################\n",
      "Episode: 140, score: 0.002074\n",
      "0.002073604014565043\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.001175\n",
      "0.0011751268772871664\n",
      "################################\n",
      "Episode: 40, score: 0.001665\n",
      "0.0016649745820742573\n",
      "################################\n",
      "Episode: 60, score: 0.001449\n",
      "0.0014492385462871965\n",
      "################################\n",
      "Episode: 80, score: 0.001289\n",
      "0.0012893400727038457\n",
      "################################\n",
      "Episode: 100, score: 0.001645\n",
      "0.001644670014000181\n",
      "################################\n",
      "Episode: 120, score: 0.001977\n",
      "0.0019771573162131804\n",
      "################################\n",
      "Episode: 140, score: 0.002411\n",
      "0.0024111674587965616\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.5, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000830\n",
      "0.0008299492200278692\n",
      "################################\n",
      "Episode: 40, score: 0.001470\n",
      "0.0014695431143612729\n",
      "################################\n",
      "Episode: 60, score: 0.001061\n",
      "0.001060913681870487\n",
      "################################\n",
      "Episode: 80, score: 0.000726\n",
      "0.000725888308648228\n",
      "################################\n",
      "Episode: 100, score: 0.000343\n",
      "0.0003426395862500377\n",
      "################################\n",
      "Episode: 120, score: 0.000921\n",
      "0.0009213197763612125\n",
      "################################\n",
      "Episode: 140, score: 0.000904\n",
      "0.0009035532792963958\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.000657\n",
      "0.0006573603913982205\n",
      "################################\n",
      "Episode: 40, score: 0.000914\n",
      "0.0009137055633334339\n",
      "################################\n",
      "Episode: 60, score: 0.001340\n",
      "0.0013401014928890365\n",
      "################################\n",
      "Episode: 80, score: 0.001096\n",
      "0.0010964466760001207\n",
      "################################\n",
      "Episode: 100, score: 0.001254\n",
      "0.001253807078574212\n",
      "################################\n",
      "Episode: 120, score: 0.001617\n",
      "0.001616751232898326\n",
      "################################\n",
      "Episode: 140, score: 0.001536\n",
      "0.001535532960602021\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000865\n",
      "0.0008654822141575027\n",
      "################################\n",
      "Episode: 40, score: 0.001127\n",
      "0.0011269035281112351\n",
      "################################\n",
      "Episode: 60, score: 0.001305\n",
      "0.0013045684987594028\n",
      "################################\n",
      "Episode: 80, score: 0.001794\n",
      "0.0017944162035464937\n",
      "################################\n",
      "Episode: 100, score: 0.001845\n",
      "0.0018451776237316845\n",
      "################################\n",
      "Episode: 120, score: 0.002020\n",
      "0.0020203045233705926\n",
      "################################\n",
      "Episode: 140, score: 0.001528\n",
      "0.0015279187475742422\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.3, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.001211\n",
      "0.0012106598714167998\n",
      "################################\n",
      "Episode: 40, score: 0.001624\n",
      "0.0016243654459261046\n",
      "################################\n",
      "Episode: 60, score: 0.001269\n",
      "0.0012690355046297693\n",
      "################################\n",
      "Episode: 80, score: 0.001754\n",
      "0.0017538070673983413\n",
      "################################\n",
      "Episode: 100, score: 0.000952\n",
      "0.000951776628472327\n",
      "################################\n",
      "Episode: 120, score: 0.001213\n",
      "0.0012131979424260593\n",
      "################################\n",
      "Episode: 140, score: 0.001033\n",
      "0.0010329949007686322\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.01}\n",
      "################################\n",
      "Episode: 20, score: 0.001264\n",
      "0.0012639593626112503\n",
      "################################\n",
      "Episode: 40, score: 0.001505\n",
      "0.0015050761084909063\n",
      "################################\n",
      "Episode: 60, score: 0.001317\n",
      "0.0013172588538057006\n",
      "################################\n",
      "Episode: 80, score: 0.001500\n",
      "0.0014999999664723873\n",
      "################################\n",
      "Episode: 100, score: 0.001617\n",
      "0.001616751232898326\n",
      "################################\n",
      "Episode: 120, score: 0.002378\n",
      "0.002378172535676188\n",
      "################################\n",
      "Episode: 140, score: 0.001952\n",
      "0.0019517766061205853\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.03}\n",
      "################################\n",
      "Episode: 20, score: 0.000799\n",
      "0.0007994923679167546\n",
      "################################\n",
      "Episode: 40, score: 0.000711\n",
      "0.0007106598825926708\n",
      "################################\n",
      "Episode: 60, score: 0.000896\n",
      "0.0008959390662686171\n",
      "################################\n",
      "Episode: 80, score: 0.001193\n",
      "0.0011928933743519832\n",
      "################################\n",
      "Episode: 100, score: 0.000975\n",
      "0.0009746192675556629\n",
      "################################\n",
      "Episode: 120, score: 0.001185\n",
      "0.0011852791613242044\n",
      "################################\n",
      "Episode: 140, score: 0.001256\n",
      "0.0012563451495834716\n",
      "{'SGD_epoch': 4, 'lr': 0.001, 'discount_rate': 0.95, 'epsilon': 0.1, 'beta': 0.06}\n",
      "################################\n",
      "Episode: 20, score: 0.000662\n",
      "0.0006624365334167396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-8a78e44003fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                             \u001b[1;31m# uncomment to utilize your own clipped function!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                             \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mclipped_surrogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                             \u001b[1;31m#L.requires_grad_() # I needed to do that to compute something but maybe that means that there is a bug.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-212e40c81587>\u001b[0m in \u001b[0;36mclipped_surrogate\u001b[1;34m(device, policy, old_probs, actions, states, rewards, discount, epsilon, beta)\u001b[0m\n\u001b[0;32m      2\u001b[0m                       discount = 0.995, epsilon=0.1, beta=0.01):\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnew_probs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNew_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mold_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(\"There is Nan new_probs\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-f1cd38162127>\u001b[0m in \u001b[0;36mNew_prob\u001b[1;34m(policy, states, actions, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Loop over the state and action (a,s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstate_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m#sample = m.sample()#.detach()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_iter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Prob on the previous action but new policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Navigation2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-f9d837b4ed95>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Tanh because action_values between -1 and 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3bis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Activation to stay always >= 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Navigation2\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Data = []\n",
    "for SGD_epoch in SGD_epoch_tab:\n",
    "    for lr in lr_tab:\n",
    "        for discount_rate in discount_rate_tab:\n",
    "            for epsilon in epsilon_tab:\n",
    "                for beta in beta_tab:\n",
    "                    param_dico = {\"SGD_epoch\":SGD_epoch,\"lr\":lr,\"discount_rate\":discount_rate,\"epsilon\":epsilon,\"beta\":beta}\n",
    "                    # get the default brain\n",
    "                    brain_name = env.brain_names[0]\n",
    "                    brain = env.brains[brain_name]\n",
    "                    env_info = env.reset(train_mode=True)[brain_name]  \n",
    "                    states = env_info.vector_observations # get the current state (for each agent\n",
    "                    num_agents = len(states)\n",
    "                    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "                    nb_states = len(states[0])\n",
    "                    action_size = brain.vector_action_space_size\n",
    "                    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    policy = Policy(nb_states,action_size).to(device)\n",
    "                    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "                    ###################################################### MAIN_CODE #################################################\n",
    "                    # training loop max iterations\n",
    "\n",
    "\n",
    "                    # widget bar to display progress\n",
    "                    #!pip install progressbar\n",
    "                    #import progressbar as pb\n",
    "                    #widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "                    #timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "                    #discount_rate = .99\n",
    "                    #epsilon = 0.5\n",
    "                    #beta = .01\n",
    "                    #tmax = 320\n",
    "                    #SGD_epoch = 6\n",
    "\n",
    "                    # keep track of progress\n",
    "                    mean_rewards = []\n",
    "                    print(param_dico)\n",
    "                    epsilon2 = epsilon\n",
    "\n",
    "                    for e in range(episode):\n",
    "\n",
    "                        # collect trajectories\n",
    "                        states, actions, rewards,prob = collect_trajectories(env,env_info, policy,device,tmax)\n",
    "                        total_rewards = np.mean(rewards)\n",
    "\n",
    "                        # gradient ascent step\n",
    "                        for _ in range(SGD_epoch):\n",
    "\n",
    "                            # uncomment to utilize your own clipped function!\n",
    "                            L = -clipped_surrogate(device,policy,prob,actions, states, rewards,discount =discount_rate, epsilon=epsilon, beta=beta)\n",
    "                            #L.requires_grad_() # I needed to do that to compute something but maybe that means that there is a bug.\n",
    "                            optimizer.zero_grad()\n",
    "                            L.backward()\n",
    "                            optimizer.step()\n",
    "                            del L\n",
    "\n",
    "                        # the clipping parameter reduces as time goes on\n",
    "                        epsilon2*=.995\n",
    "\n",
    "                        # the regulation term also reduces\n",
    "                        # this reduces exploration in later runs\n",
    "                        beta*=.995\n",
    "\n",
    "                        # get the average reward of the parallel environments\n",
    "                        mean_rewards.append(total_rewards)\n",
    "\n",
    "                        # display some progress every 20 iterations\n",
    "                        if (e+1)%20 ==0 :\n",
    "                            print(\"################################\")\n",
    "                            print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "                            print(total_rewards)\n",
    "                    Data.append([param_dico,mean_rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001469245523751156\n"
     ]
    }
   ],
   "source": [
    "# \"SGD_epoch\":SGD_epoch,\"lr\":lr,\"discount_rate\":discount_rate,\"epsilon\":epsilon,\"beta\":beta}\n",
    "Databis = Data\n",
    "Databis = np.asarray(Databis)\n",
    "Moyenne = []\n",
    "mean_iter = 50\n",
    "Var = \"beta\"\n",
    "Value = 0.06\n",
    "for i,data in enumerate(Databis):\n",
    "    for key, value in data[0].items():\n",
    "        if (key == Var and value == Value):\n",
    "            Moyenne.append(np.mean(data[1][130:]))\n",
    "Moyenne = np.asarray(Moyenne)\n",
    "print(np.mean(Moyenne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount rate = 0.99 max\n",
    "# lr = 0.001 max\n",
    "# epsilon = 0.1 min\n",
    "# beta = 0.03 medium (moins clair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Hyp_searchPPO.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(Data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenn update the paramters\n",
    "# Get the distribution\n",
    "# Making the entropy term\n",
    "# log_prob -> Distrib(log_prob(a)) # Avec a -> a[perm].clone() # From Make Batch # From\n",
    "# When evaluation -> :#only used when evaluate the policy.Making the performance more stable\n",
    "# When select action -> a = self.actor.dist_mode(state)\n",
    "# EVALUATE -> Proba =0\n",
    "# Select_action -> Interact with the environment \n",
    "\n",
    "# WHAT TO DO\n",
    "# Render = FALSE -> Select_action a, log_prob(a) -> Don't put in Clip -> Put inside the memory.\n",
    "# On a a et log_prob(a)\n",
    "\n",
    "# Training : \n",
    "# -> States \n",
    "# Actor -> Get_dist : mu,sigma = forward(state)\n",
    "# dist=  Normal(mu,sigma)\n",
    "# return Distrib\n",
    "# log_prob_a_now -> Distrib(log_prob(a[index])) OK \n",
    "\n",
    "\n",
    "# This function is differentiable, so gradients will flow back from the result of this operation to input.\n",
    "# For -> s, a, td_target, adv, logprob_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One style of policy gradient implementation, popularized in [Mni+16] and well-suited for use\n",
    "#with recurrent neural networks, runs the policy for T timesteps (where T is much less than the\n",
    "#episode length), and uses the collected samples for an update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each iteration, each of N (parallel) actors coThen we\n",
    "#construct the surrogate loss on these NT timesteps of data, and optimize it with minibatch SGD\n",
    "#(or usually for better performance, Adam [KB14]), for K epochs.llect T timesteps of data.\n",
    "# Optimize surrogate L wrt, with K epochs and minibatch size M  NT\n",
    "\n",
    "# Clipping = 0.2\n",
    "# Horizon (2047) 512\n",
    "# Minibatch size = 64, 4096\n",
    "# Log stdev of action distribution LinearAnneal (-0.7,-1.6)\n",
    "# GAE parameter 0.95\n",
    "# 3x10-4\n",
    "# Discount 0.99\n",
    "# Num_epoch entre 10 et 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficult to put the good beta for entropy rate\n",
    "# Decay Rate of Variance <- Difficult Variance of the Normal Law\n",
    "# Strong interaction of epsilon and N -> Seems good with very low Epsilon and N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT ACTOR-CRITIC\n",
    "# ACTOR-CRITIC CALCA\n",
    "# IMPLEMENT REPLAY BUFFER\n",
    "# IMPLEMENT CMA-ES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Navigation2",
   "language": "python",
   "name": "navigation2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
